{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4181784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mqtt_ids_boost_train.py\n",
    "# Batch + epoch training of a boosted decision tree (XGBoost) for multi-class attack-type detection.\n",
    "# Processes packet / uniflow / biflow CSVs in streaming mode (chunks), creates a multi-class\n",
    "# \"attack_type\" target (normal is one type), trains incrementally across batches and epochs,\n",
    "# validates per-epoch on an in-memory sampled validation set, and evaluates finally on test set.\n",
    "#\n",
    "# IMPORTANT: adjust base_path and folders/filenames to point to your CSV files.\n",
    "# This script deletes only temporary files it creates and does an additional cleanup of common temp files.\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import tempfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import xgboost as xgb\n",
    "import random\n",
    "\n",
    "# ------------------------------\n",
    "# Config - adjust these paths & hyperparams\n",
    "# ------------------------------\n",
    "base_path = \"./\"   # base folder where \"packet_features\", \"uniflow_features\", \"biflow_features\" reside\n",
    "folders = {\n",
    "    \"packet\": \"packet_features\",\n",
    "    \"uniflow\": \"uniflow_features\",\n",
    "    \"biflow\": \"biflow_features\"\n",
    "}\n",
    "\n",
    "# filenames expected in each folder (same names used by your earlier script)\n",
    "files = {\n",
    "    \"normal\": \"normal.csv\",\n",
    "    \"sparta\": \"sparta.csv\",\n",
    "    \"scan_A\": \"scan_A.csv\",\n",
    "    \"mqtt_bruteforce\": \"mqtt_bruteforce.csv\",\n",
    "    \"scan_sU\": \"scan_sU.csv\"\n",
    "}\n",
    "\n",
    "# If your uniflow/biflow files are prefixed differently, you can change these helper names\n",
    "def build_filenames(prefix):\n",
    "    return {\n",
    "        \"normal\": f\"{prefix}_normal.csv\",\n",
    "        \"sparta\": f\"{prefix}_sparta.csv\",\n",
    "        \"scan_A\": f\"{prefix}_scan_A.csv\",\n",
    "        \"mqtt_bruteforce\": f\"{prefix}_mqtt_bruteforce.csv\",\n",
    "        \"scan_sU\": f\"{prefix}_scan_sU.csv\"\n",
    "    }\n",
    "\n",
    "feature_files = {\n",
    "    \"packet\": files,\n",
    "    \"uniflow\": build_filenames(\"uniflow\"),\n",
    "    \"biflow\": build_filenames(\"biflow\")\n",
    "}\n",
    "\n",
    "# training hyperparams\n",
    "CHUNKSIZE = 200000           # number of rows per CSV chunk read\n",
    "TRAIN_FRACTION = 0.80\n",
    "VAL_FRACTION = 0.10\n",
    "TEST_FRACTION = 0.10\n",
    "\n",
    "EPOCHS = 3                   # how many passes over the entire dataset\n",
    "ROUNDS_PER_BATCH = 1         # xgboost boosting rounds per batch update (small number for fine updates)\n",
    "XGB_PARAMS = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"eta\": 0.1,\n",
    "    \"max_depth\": 6,\n",
    "    \"verbosity\": 0,\n",
    "    # \"tree_method\": \"hist\"  # optionally set depending on your environment\n",
    "}\n",
    "\n",
    "SAMPLE_VAL_MAX = 20000       # keep at most this many rows in validation set (sampled)\n",
    "SAMPLE_TEST_MAX = 20000      # same for test set\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# ------------------------------\n",
    "# Utilities & bookkeeping\n",
    "# ------------------------------\n",
    "TEMP_FILES_CREATED = []   # track any temp files we create so we can delete them later\n",
    "\n",
    "def safe_remove(path):\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)\n",
    "            print(f\"[CLEANUP] Removed file: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[CLEANUP] Could not remove {path}: {e}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Build a mapping from filename key to textual attack type (target)\n",
    "# ------------------------------\n",
    "attack_type_names = [\"normal\", \"sparta\", \"scan_A\", \"mqtt_bruteforce\", \"scan_sU\"]\n",
    "# label encoder will convert these to ints later\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(attack_type_names)\n",
    "\n",
    "# ------------------------------\n",
    "# Streaming data generator\n",
    "# ------------------------------\n",
    "def stream_chunks_for_all_files(feature_files_map, base_path, chunksize=CHUNKSIZE):\n",
    "    \"\"\"\n",
    "    Generator that yields tuples: (level, file_key, filepath, chunk_df)\n",
    "    Iterates over feature levels and their files, reading each CSV in chunks.\n",
    "    \"\"\"\n",
    "    for level, file_dict in feature_files_map.items():\n",
    "        folder_path = os.path.join(base_path, folders[level])\n",
    "        for key, fname in file_dict.items():\n",
    "            fpath = os.path.join(folder_path, fname)\n",
    "            if not os.path.isfile(fpath):\n",
    "                print(f\"[WARN] File not found: {fpath}. Skipping.\")\n",
    "                continue\n",
    "            try:\n",
    "                # pandas read_csv with chunksize\n",
    "                for chunk in pd.read_csv(fpath, chunksize=chunksize, low_memory=False):\n",
    "                    yield (level, key, fpath, chunk)\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(f\"[WARN] Empty CSV: {fpath}. Skipping.\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Reading {fpath} failed: {e}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Preprocess a chunk:\n",
    "# - add attack_type target column (text), then encoded label (int)\n",
    "# - select numeric columns only (drop non-numeric / non-finite)\n",
    "# - fill NaNs with zero (or another strategy)\n",
    "# - return X (ndarray) and y (ndarray) plus feature_names\n",
    "# ------------------------------\n",
    "def preprocess_chunk(df, file_key, expected_feature_names=None):\n",
    "    \"\"\"\n",
    "    df: pandas DataFrame chunk\n",
    "    file_key: filename key like 'normal', 'sparta', ...\n",
    "    expected_feature_names: if provided, will attempt to reorder/select features to match\n",
    "    returns: X, y, feature_names\n",
    "    \"\"\"\n",
    "    # add textual attack type column and numeric encoded label\n",
    "    df = df.copy()\n",
    "\n",
    "    # Create attack_type (textual)\n",
    "    df[\"attack_type\"] = file_key\n",
    "\n",
    "    # If there's already a 'label' or 'attack_type' column from earlier pipelines, prefer the textual name we set\n",
    "    # Convert label to categorical integer using our label_encoder mapping\n",
    "    try:\n",
    "        y_text = df[\"attack_type\"].astype(str).values\n",
    "        y_encoded = label_encoder.transform(y_text)\n",
    "    except Exception as e:\n",
    "        # If label values are something else, map using dictionary fallback\n",
    "        mapping = {name: idx for idx, name in enumerate(label_encoder.classes_)}\n",
    "        y_encoded = df[\"attack_type\"].map(mapping).fillna(-1).astype(int).values\n",
    "\n",
    "    # Select numeric columns only for features\n",
    "    numeric_df = df.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "    # Drop potential target-like numeric columns if they exist in numeric set (avoid leakage)\n",
    "    for col in [\"label\", \"attack_type\"]:\n",
    "        if col in numeric_df.columns:\n",
    "            numeric_df = numeric_df.drop(columns=[col])\n",
    "\n",
    "    # If expected_feature_names provided, we intersect to maintain consistent features across batches\n",
    "    if expected_feature_names is not None:\n",
    "        # keep only columns that are in expected_feature_names; order them accordingly\n",
    "        intersect = [c for c in expected_feature_names if c in numeric_df.columns]\n",
    "        numeric_df = numeric_df[intersect]\n",
    "        feature_names = intersect\n",
    "    else:\n",
    "        feature_names = list(numeric_df.columns)\n",
    "\n",
    "    # Fill missing values\n",
    "    if len(numeric_df.columns) == 0:\n",
    "        # No numeric features available -> raise\n",
    "        raise ValueError(\"No numeric features found in this chunk. Consider checking CSV format.\")\n",
    "    numeric_df = numeric_df.fillna(0.0)\n",
    "\n",
    "    X = numeric_df.values.astype(np.float32)\n",
    "    y = y_encoded.astype(np.int32)\n",
    "\n",
    "    return X, y, feature_names\n",
    "\n",
    "# ------------------------------\n",
    "# Main training loop\n",
    "# ------------------------------\n",
    "def train_boosted_tree_batchwise(feature_files_map, base_path, epochs=EPOCHS):\n",
    "    \"\"\"\n",
    "    Trains an XGBoost booster incrementally over batches and epochs.\n",
    "    Returns final booster and test set performance.\n",
    "    \"\"\"\n",
    "    booster = None\n",
    "    feature_names_master = None  # features selected from first processed chunk\n",
    "    val_X = None; val_y = None\n",
    "    test_X = None; test_y = None\n",
    "\n",
    "    total_batches = 0\n",
    "\n",
    "    # We will perform epochs: for each epoch we iterate through all chunks via stream generator\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start = time.time()\n",
    "        print(f\"\\n=== EPOCH {epoch}/{epochs} ===\")\n",
    "\n",
    "        # iterate all chunks (fresh generator)\n",
    "        gen = stream_chunks_for_all_files(feature_files_map, base_path)\n",
    "        for level, file_key, filepath, chunk in gen:\n",
    "            total_batches += 1\n",
    "            batch_id_str = f\"ep{epoch}_b{total_batches}\"\n",
    "            print(f\"[BATCH {batch_id_str}] level={level} file_key={file_key} shape={chunk.shape}\")\n",
    "\n",
    "            # Preprocess: determine or reuse master feature names\n",
    "            try:\n",
    "                if feature_names_master is None:\n",
    "                    # first chunk we process: establish feature set\n",
    "                    X_batch, y_batch, feature_names = preprocess_chunk(chunk, file_key, expected_feature_names=None)\n",
    "                    feature_names_master = feature_names\n",
    "                    print(f\"[INFO] Master feature count: {len(feature_names_master)}\")\n",
    "                else:\n",
    "                    # reuse master features for consistency\n",
    "                    X_batch, y_batch, _ = preprocess_chunk(chunk, file_key, expected_feature_names=feature_names_master)\n",
    "            except ValueError as e:\n",
    "                print(f\"[SKIP] {e} -- skipping this chunk.\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Preprocessing failed: {e} -- skipping chunk.\")\n",
    "                continue\n",
    "\n",
    "            # Now split within this batch into train/val/test indices\n",
    "            n_rows = X_batch.shape[0]\n",
    "            if n_rows == 0:\n",
    "                continue\n",
    "            rnd = np.random.rand(n_rows)\n",
    "            train_mask = rnd < TRAIN_FRACTION\n",
    "            val_mask = (rnd >= TRAIN_FRACTION) & (rnd < TRAIN_FRACTION + VAL_FRACTION)\n",
    "            test_mask = (rnd >= TRAIN_FRACTION + VAL_FRACTION)\n",
    "\n",
    "            X_train = X_batch[train_mask]\n",
    "            y_train = y_batch[train_mask]\n",
    "            X_val_chunk = X_batch[val_mask]\n",
    "            y_val_chunk = y_batch[val_mask]\n",
    "            X_test_chunk = X_batch[test_mask]\n",
    "            y_test_chunk = y_batch[test_mask]\n",
    "\n",
    "            # Append a **sample** of validation & test chunk to in-memory val/test pools to limit memory\n",
    "            if X_val_chunk.shape[0] > 0:\n",
    "                if val_X is None:\n",
    "                    # sample if too big\n",
    "                    take = min(X_val_chunk.shape[0], SAMPLE_VAL_MAX)\n",
    "                    idxs = np.random.choice(X_val_chunk.shape[0], size=take, replace=False)\n",
    "                    val_X = X_val_chunk[idxs]; val_y = y_val_chunk[idxs]\n",
    "                else:\n",
    "                    # append and trim to max\n",
    "                    combined_X = np.vstack([val_X, X_val_chunk])\n",
    "                    combined_y = np.concatenate([val_y, y_val_chunk])\n",
    "                    # if exceed SAMPLE_VAL_MAX, downsample\n",
    "                    if combined_X.shape[0] > SAMPLE_VAL_MAX:\n",
    "                        keep_idxs = np.random.choice(combined_X.shape[0], SAMPLE_VAL_MAX, replace=False)\n",
    "                        val_X = combined_X[keep_idxs]; val_y = combined_y[keep_idxs]\n",
    "                    else:\n",
    "                        val_X = combined_X; val_y = combined_y\n",
    "\n",
    "            if X_test_chunk.shape[0] > 0:\n",
    "                if test_X is None:\n",
    "                    take = min(X_test_chunk.shape[0], SAMPLE_TEST_MAX)\n",
    "                    idxs = np.random.choice(X_test_chunk.shape[0], size=take, replace=False)\n",
    "                    test_X = X_test_chunk[idxs]; test_y = y_test_chunk[idxs]\n",
    "                else:\n",
    "                    combined_X = np.vstack([test_X, X_test_chunk])\n",
    "                    combined_y = np.concatenate([test_y, y_test_chunk])\n",
    "                    if combined_X.shape[0] > SAMPLE_TEST_MAX:\n",
    "                        keep_idxs = np.random.choice(combined_X.shape[0], SAMPLE_TEST_MAX, replace=False)\n",
    "                        test_X = combined_X[keep_idxs]; test_y = combined_y[keep_idxs]\n",
    "                    else:\n",
    "                        test_X = combined_X; test_y = combined_y\n",
    "\n",
    "            # If no training rows in this batch, skip training update\n",
    "            if X_train.shape[0] == 0:\n",
    "                print(f\"[BATCH {batch_id_str}] no training rows, skipping update.\")\n",
    "                continue\n",
    "\n",
    "            # Build DMatrix\n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=feature_names_master)\n",
    "\n",
    "            # Train/update booster incrementally\n",
    "            # We call xgb.train with xgb_model=booster to continue training from prior booster (or None)\n",
    "            try:\n",
    "                booster = xgb.train(\n",
    "                    params={**XGB_PARAMS, \"num_class\": len(label_encoder.classes_)},\n",
    "                    dtrain=dtrain,\n",
    "                    num_boost_round=ROUNDS_PER_BATCH,\n",
    "                    xgb_model=booster,\n",
    "                    verbose_eval=False\n",
    "                )\n",
    "                print(f\"[BATCH {batch_id_str}] booster updated with {X_train.shape[0]} training rows.\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] xgboost.train failed on batch {batch_id_str}: {e}\")\n",
    "\n",
    "        # End of epoch: evaluate on validation pool if available\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"--- Completed epoch {epoch} in {epoch_time:.1f}s. Running validation...\")\n",
    "\n",
    "        if val_X is not None and val_X.shape[0] > 0 and booster is not None:\n",
    "            dval = xgb.DMatrix(val_X, label=val_y, feature_names=feature_names_master)\n",
    "            val_preds_prob = booster.predict(dval)  # shape (n_samples, num_class)\n",
    "            val_preds = np.argmax(val_preds_prob, axis=1)\n",
    "            acc = accuracy_score(val_y, val_preds)\n",
    "            f1 = f1_score(val_y, val_preds, average=\"macro\")\n",
    "            print(f\"[EPOCH {epoch}] Validation Accuracy: {acc:.4f}  Macro-F1: {f1:.4f}\")\n",
    "        else:\n",
    "            print(f\"[EPOCH {epoch}] No validation data or booster unavailable to evaluate.\")\n",
    "\n",
    "    # Done all epochs -> final evaluation on test set\n",
    "    if test_X is not None and test_X.shape[0] > 0 and booster is not None:\n",
    "        dtest = xgb.DMatrix(test_X, label=test_y, feature_names=feature_names_master)\n",
    "        test_probs = booster.predict(dtest)\n",
    "        test_preds = np.argmax(test_probs, axis=1)\n",
    "        test_acc = accuracy_score(test_y, test_preds)\n",
    "        test_f1 = f1_score(test_y, test_preds, average=\"macro\")\n",
    "        report = classification_report(test_y, test_preds, target_names=label_encoder.classes_, zero_division=0)\n",
    "        print(f\"\\n=== FINAL TEST METRICS ===\")\n",
    "        print(f\"Test Accuracy: {test_acc:.4f}  Macro-F1: {test_f1:.4f}\")\n",
    "        print(\"Classification report:\\n\")\n",
    "        print(report)\n",
    "    else:\n",
    "        print(\"No test data or booster unavailable; skipping final test evaluation.\")\n",
    "\n",
    "    return booster, feature_names_master\n",
    "\n",
    "# ------------------------------\n",
    "# Run training\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    start_all = time.time()\n",
    "    print(\"[START] Batch-epoch boosting training started.\")\n",
    "    try:\n",
    "        final_booster, final_features = train_boosted_tree_batchwise(feature_files, base_path, epochs=EPOCHS)\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Training aborted due to error: {e}\")\n",
    "        final_booster = None\n",
    "        final_features = None\n",
    "\n",
    "    elapsed_total = time.time() - start_all\n",
    "    print(f\"[DONE] Total elapsed time: {elapsed_total:.1f} seconds\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Save final booster to disk temporarily and then remove (if user wants persistent model, comment out removal)\n",
    "    # ------------------------------\n",
    "    try:\n",
    "        if final_booster is not None:\n",
    "            tmp_model_path = os.path.join(tempfile.gettempdir(), f\"xgb_booster_{int(time.time())}.model\")\n",
    "            final_booster.save_model(tmp_model_path)\n",
    "            print(f\"[INFO] Saved booster temporarily to {tmp_model_path}\")\n",
    "            TEMP_FILES_CREATED.append(tmp_model_path)\n",
    "            # If you want to persist model, move it to a permanent path and remove deletion below.\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not save booster: {e}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Cleanup: remove temporary files we created + common tmp files of pattern *.tmp / *.temp in temp dir\n",
    "    # ------------------------------\n",
    "    print(\"\\n[START CLEANUP] Removing temporary files created during processing...\")\n",
    "    for f in list(TEMP_FILES_CREATED):\n",
    "        safe_remove(f)\n",
    "    # Also try to clean common temp extensions in system temp dir\n",
    "    tmpdir = tempfile.gettempdir()\n",
    "    for pattern in (\"*.tmp\", \"*.temp\", \"tmp*\"):\n",
    "        for f in glob.glob(os.path.join(tmpdir, pattern)):\n",
    "            try:\n",
    "                # only remove files, not directories\n",
    "                if os.path.isfile(f):\n",
    "                    os.remove(f)\n",
    "                    print(f\"[CLEANUP] Removed temp file: {f}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    print(\"[CLEANUP] Completed temporary file cleanup.\")\n",
    "    print(\"[EXIT] Script finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seawsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
