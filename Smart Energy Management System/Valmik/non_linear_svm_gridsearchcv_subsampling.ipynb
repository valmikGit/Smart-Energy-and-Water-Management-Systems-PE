{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6665ed",
   "metadata": {},
   "source": [
    "With Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c39224a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Training RBF/Linear SVM with GridSearchCV (epochs=3)...\n",
      "\n",
      "=== EPOCH 1/3 ===\n",
      "[INFO] Downsampled training set to 5000 samples.\n",
      "[EPOCH 1] Running GridSearchCV (cv=3) ...\n",
      "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n",
      "[EPOCH 1] Best params: {'C': 10, 'class_weight': None, 'gamma': 'scale', 'kernel': 'linear'}  CV f1_macro=0.8004\n",
      "[EPOCH 1] Validation: Acc=0.7991  F1_macro=0.4442\n",
      "[EPOCH 1] Test: Acc=0.7997  F1_macro=0.4444\n",
      "\n",
      "=== EPOCH 2/3 ===\n",
      "[INFO] Downsampled training set to 5000 samples.\n",
      "[EPOCH 2] Running GridSearchCV (cv=3) ...\n",
      "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n",
      "[EPOCH 2] Best params: {'C': 10, 'class_weight': None, 'gamma': 1, 'kernel': 'rbf'}  CV f1_macro=0.7582\n",
      "[EPOCH 2] Validation: Acc=0.8015  F1_macro=0.4449\n",
      "[EPOCH 2] Test: Acc=0.8012  F1_macro=0.4448\n",
      "\n",
      "=== EPOCH 3/3 ===\n",
      "[INFO] Downsampled training set to 5000 samples.\n",
      "[EPOCH 3] Running GridSearchCV (cv=3) ...\n",
      "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n",
      "[EPOCH 3] Best params: {'C': 1000, 'class_weight': None, 'gamma': 0.001, 'kernel': 'rbf'}  CV f1_macro=0.7616\n",
      "[EPOCH 3] Validation: Acc=0.8011  F1_macro=0.4448\n",
      "[EPOCH 3] Test: Acc=0.8007  F1_macro=0.4447\n",
      "\n",
      "=== FINAL EVALUATION ON TEST POOL ===\n",
      "Test Acc=0.8007  F1_macro=0.4447\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Secure(False)       0.00      0.00      0.00      3671\n",
      " Attack(True)       0.81      0.98      0.89     16329\n",
      "\n",
      "     accuracy                           0.80     20000\n",
      "    macro avg       0.41      0.49      0.44     20000\n",
      " weighted avg       0.66      0.80      0.73     20000\n",
      "\n",
      "[DONE] Total time: 1171.4s\n",
      "[RESULT] Best params (last epoch): {'C': 1000, 'class_weight': None, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "[START CLEANUP]\n",
      "[CLEANUP] Could not remove C:\\Users\\HP\\AppData\\Local\\Temp\\1e918a4d-62c9-4c43-a259-262cca18d98f.tmp: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\HP\\\\AppData\\\\Local\\\\Temp\\\\1e918a4d-62c9-4c43-a259-262cca18d98f.tmp'\n",
      "[CLEANUP] Could not remove C:\\Users\\HP\\AppData\\Local\\Temp\\2abe425b-aeac-49c5-8950-aaffb4952758.tmp: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\HP\\\\AppData\\\\Local\\\\Temp\\\\2abe425b-aeac-49c5-8950-aaffb4952758.tmp'\n",
      "[CLEANUP] Could not remove C:\\Users\\HP\\AppData\\Local\\Temp\\39462418-c736-4e28-aef8-ca9597f4debe.tmp: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\HP\\\\AppData\\\\Local\\\\Temp\\\\39462418-c736-4e28-aef8-ca9597f4debe.tmp'\n",
      "[CLEANUP] Could not remove C:\\Users\\HP\\AppData\\Local\\Temp\\44cda7fb-f7a0-4583-9592-ac5fbda4ce08.tmp: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\HP\\\\AppData\\\\Local\\\\Temp\\\\44cda7fb-f7a0-4583-9592-ac5fbda4ce08.tmp'\n",
      "[CLEANUP] Could not remove C:\\Users\\HP\\AppData\\Local\\Temp\\5ec176f8-704c-4c65-9a84-0d69b0c81ef3.tmp: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\HP\\\\AppData\\\\Local\\\\Temp\\\\5ec176f8-704c-4c65-9a84-0d69b0c81ef3.tmp'\n",
      "[CLEANUP] Could not remove C:\\Users\\HP\\AppData\\Local\\Temp\\a618f8c9-b108-465b-941a-bf8ea893f81d.tmp: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\HP\\\\AppData\\\\Local\\\\Temp\\\\a618f8c9-b108-465b-941a-bf8ea893f81d.tmp'\n",
      "[CLEANUP] Could not remove C:\\Users\\HP\\AppData\\Local\\Temp\\bb807153-5f27-4b13-8174-68dc57586220.tmp: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\HP\\\\AppData\\\\Local\\\\Temp\\\\bb807153-5f27-4b13-8174-68dc57586220.tmp'\n",
      "[CLEANUP] Could not remove C:\\Users\\HP\\AppData\\Local\\Temp\\e153bcc0-3bce-4a44-9af5-117ac2a511b6.tmp: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\HP\\\\AppData\\\\Local\\\\Temp\\\\e153bcc0-3bce-4a44-9af5-117ac2a511b6.tmp'\n",
      "[CLEANUP] Could not remove C:\\Users\\HP\\AppData\\Local\\Temp\\e29a11b4-7e20-4927-8f40-81b61f8392a3.tmp: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\HP\\\\AppData\\\\Local\\\\Temp\\\\e29a11b4-7e20-4927-8f40-81b61f8392a3.tmp'\n",
      "[CLEANUP] Could not remove C:\\Users\\HP\\AppData\\Local\\Temp\\e7004216-ebdd-435a-bd38-c54f3df9989b.tmp: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\HP\\\\AppData\\\\Local\\\\Temp\\\\e7004216-ebdd-435a-bd38-c54f3df9989b.tmp'\n",
      "[CLEANUP] Completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import random\n",
    "\n",
    "# ------------------------------\n",
    "# Config\n",
    "# ------------------------------\n",
    "base_path = \"./\"\n",
    "\n",
    "folders = {\n",
    "    \"packet\": \"packet_features\",\n",
    "    \"uniflow\": \"uniflow_features\",\n",
    "    \"biflow\": \"biflow_features\"\n",
    "}\n",
    "\n",
    "files = {\n",
    "    \"normal\": \"normal.csv\",\n",
    "    \"sparta\": \"sparta.csv\",\n",
    "    \"scan_A\": \"scan_A.csv\",\n",
    "    \"mqtt_bruteforce\": \"mqtt_bruteforce.csv\",\n",
    "    \"scan_sU\": \"scan_sU.csv\"\n",
    "}\n",
    "\n",
    "def build_filenames(prefix):\n",
    "    return {\n",
    "        \"normal\": f\"{prefix}_normal.csv\",\n",
    "        \"sparta\": f\"{prefix}_sparta.csv\",\n",
    "        \"scan_A\": f\"{prefix}_scan_A.csv\",\n",
    "        \"mqtt_bruteforce\": f\"{prefix}_mqtt_bruteforce.csv\",\n",
    "        \"scan_sU\": f\"{prefix}_scan_sU.csv\"\n",
    "    }\n",
    "\n",
    "feature_files = {\n",
    "    \"packet\": files,\n",
    "    \"uniflow\": build_filenames(\"uniflow\"),\n",
    "    \"biflow\": build_filenames(\"biflow\")\n",
    "}\n",
    "\n",
    "CHUNKSIZE = 200000\n",
    "TRAIN_FRACTION = 0.80\n",
    "VAL_FRACTION = 0.10\n",
    "TEST_FRACTION = 0.10\n",
    "\n",
    "EPOCHS = 3\n",
    "SAMPLE_VAL_MAX = 20000\n",
    "SAMPLE_TEST_MAX = 20000\n",
    "\n",
    "# ðŸ”¹ Increased training cap for better accuracy\n",
    "MAX_TRAIN_SAMPLES = 5000\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "CV_FOLDS = 3\n",
    "SCORING = \"f1_macro\"\n",
    "\n",
    "# Expanded hyperparameter grid\n",
    "GRID_PARAM_GRID = {\n",
    "    \"C\": [0.1, 1, 10, 100, 1000],\n",
    "    \"gamma\": [\"scale\", \"auto\", 1, 0.1, 0.01, 0.001],\n",
    "    \"kernel\": [\"rbf\", \"linear\"],\n",
    "    \"class_weight\": [None, \"balanced\"]\n",
    "}\n",
    "\n",
    "# ------------------------------\n",
    "# Helpers\n",
    "# ------------------------------\n",
    "def safe_remove(path):\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)\n",
    "            print(f\"[CLEANUP] Removed file: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[CLEANUP] Could not remove {path}: {e}\")\n",
    "\n",
    "def stream_chunks(feature_files_map, base_path, chunksize=CHUNKSIZE):\n",
    "    \"\"\"Yield (level, file_key, filepath, chunk_df).\"\"\"\n",
    "    for level, file_dict in feature_files_map.items():\n",
    "        folder_path = os.path.join(base_path, folders[level])\n",
    "        for key, fname in file_dict.items():\n",
    "            fpath = os.path.join(folder_path, fname)\n",
    "            if not os.path.isfile(fpath):\n",
    "                print(f\"[WARN] Missing: {fpath}\")\n",
    "                continue\n",
    "            try:\n",
    "                for chunk in pd.read_csv(fpath, chunksize=chunksize, low_memory=False):\n",
    "                    yield (level, key, fpath, chunk)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to read {fpath}: {e}\")\n",
    "\n",
    "def preprocess_chunk(df, file_key, expected_features=None):\n",
    "    \"\"\"\n",
    "    Preprocess chunk:\n",
    "      - set attack bool target column 'attack'\n",
    "      - select numeric features, drop 'attack' or 'label'\n",
    "      - align to expected_features if provided\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"attack\"] = (file_key != \"normal\")\n",
    "\n",
    "    y = df[\"attack\"].astype(int).values\n",
    "    numeric_df = df.select_dtypes(include=[np.number]).copy()\n",
    "    if \"attack\" in numeric_df.columns:\n",
    "        numeric_df = numeric_df.drop(columns=[\"attack\"])\n",
    "    if \"label\" in numeric_df.columns:\n",
    "        numeric_df = numeric_df.drop(columns=[\"label\"])\n",
    "\n",
    "    if expected_features is not None:\n",
    "        numeric_df = numeric_df.reindex(columns=expected_features, fill_value=0.0)\n",
    "        feature_names = expected_features\n",
    "    else:\n",
    "        feature_names = list(numeric_df.columns)\n",
    "\n",
    "    if len(feature_names) == 0:\n",
    "        X = np.zeros((numeric_df.shape[0], 0), dtype=np.float32)\n",
    "    else:\n",
    "        X = numeric_df.fillna(0.0).values.astype(np.float32)\n",
    "\n",
    "    return X, y, feature_names\n",
    "\n",
    "# ------------------------------\n",
    "# Training with stable pools + GridSearchCV\n",
    "# ------------------------------\n",
    "def train_svm_rbf_grid(feature_files_map, base_path, epochs=EPOCHS):\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "\n",
    "    feature_names_master = None\n",
    "    val_X = None; val_y = None\n",
    "    test_X = None; test_y = None\n",
    "\n",
    "    best_model = None\n",
    "    best_params_overall = None\n",
    "\n",
    "    # ------------------------------\n",
    "    # Build stable validation/test pools ONCE\n",
    "    # ------------------------------\n",
    "    train_chunks_X = []\n",
    "    train_chunks_y = []\n",
    "    for level, file_key, filepath, chunk in stream_chunks(feature_files_map, base_path):\n",
    "        try:\n",
    "            if feature_names_master is None:\n",
    "                Xc, yc, feat_names = preprocess_chunk(chunk, file_key, expected_features=None)\n",
    "                feature_names_master = feat_names\n",
    "            else:\n",
    "                Xc, yc, _ = preprocess_chunk(chunk, file_key, expected_features=feature_names_master)\n",
    "        except Exception as e:\n",
    "            print(f\"[SKIP] Preprocess error: {e}\")\n",
    "            continue\n",
    "\n",
    "        if Xc.shape[1] == 0 or Xc.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        n = Xc.shape[0]\n",
    "        rnd = np.random.rand(n)\n",
    "        train_mask = rnd < TRAIN_FRACTION\n",
    "        val_mask = (rnd >= TRAIN_FRACTION) & (rnd < TRAIN_FRACTION + VAL_FRACTION)\n",
    "        test_mask = rnd >= TRAIN_FRACTION + VAL_FRACTION\n",
    "\n",
    "        X_train_chunk = Xc[train_mask]; y_train_chunk = yc[train_mask]\n",
    "        X_val_chunk = Xc[val_mask]; y_val_chunk = yc[val_mask]\n",
    "        X_test_chunk = Xc[test_mask]; y_test_chunk = yc[test_mask]\n",
    "\n",
    "        if X_train_chunk.shape[0] > 0:\n",
    "            train_chunks_X.append(X_train_chunk)\n",
    "            train_chunks_y.append(y_train_chunk)\n",
    "\n",
    "        if X_val_chunk.shape[0] > 0:\n",
    "            if val_X is None:\n",
    "                take = min(X_val_chunk.shape[0], SAMPLE_VAL_MAX)\n",
    "                idxs = np.random.choice(X_val_chunk.shape[0], take, replace=False)\n",
    "                val_X = X_val_chunk[idxs]; val_y = y_val_chunk[idxs]\n",
    "            else:\n",
    "                val_X = np.vstack([val_X, X_val_chunk])\n",
    "                val_y = np.concatenate([val_y, y_val_chunk])\n",
    "                if val_X.shape[0] > SAMPLE_VAL_MAX:\n",
    "                    idxs = np.random.choice(val_X.shape[0], SAMPLE_VAL_MAX, replace=False)\n",
    "                    val_X = val_X[idxs]; val_y = val_y[idxs]\n",
    "\n",
    "        if X_test_chunk.shape[0] > 0:\n",
    "            if test_X is None:\n",
    "                take = min(X_test_chunk.shape[0], SAMPLE_TEST_MAX)\n",
    "                idxs = np.random.choice(X_test_chunk.shape[0], take, replace=False)\n",
    "                test_X = X_test_chunk[idxs]; test_y = y_test_chunk[idxs]\n",
    "            else:\n",
    "                test_X = np.vstack([test_X, X_test_chunk])\n",
    "                test_y = np.concatenate([test_y, y_test_chunk])\n",
    "                if test_X.shape[0] > SAMPLE_TEST_MAX:\n",
    "                    idxs = np.random.choice(test_X.shape[0], SAMPLE_TEST_MAX, replace=False)\n",
    "                    test_X = test_X[idxs]; test_y = test_y[idxs]\n",
    "\n",
    "    # Combine all training data\n",
    "    if len(train_chunks_X) == 0:\n",
    "        print(\"[FATAL] No training data collected.\")\n",
    "        return None, None, None\n",
    "\n",
    "    X_train_full = np.vstack(train_chunks_X)\n",
    "    y_train_full = np.concatenate(train_chunks_y)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Epoch loop (with resampling for train only)\n",
    "    # ------------------------------\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\n=== EPOCH {epoch}/{epochs} ===\")\n",
    "\n",
    "        # Downsample train each epoch\n",
    "        if X_train_full.shape[0] > MAX_TRAIN_SAMPLES:\n",
    "            idxs = np.random.choice(X_train_full.shape[0], MAX_TRAIN_SAMPLES, replace=False)\n",
    "            X_train = X_train_full[idxs]\n",
    "            y_train = y_train_full[idxs]\n",
    "            print(f\"[INFO] Downsampled training set to {MAX_TRAIN_SAMPLES} samples.\")\n",
    "        else:\n",
    "            X_train = X_train_full\n",
    "            y_train = y_train_full\n",
    "\n",
    "        # Scale consistently\n",
    "        scaler.fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        val_X_scaled = scaler.transform(val_X) if val_X is not None else None\n",
    "        test_X_scaled = scaler.transform(test_X) if test_X is not None else None\n",
    "\n",
    "        # ------------------------------\n",
    "        # GridSearchCV tuning\n",
    "        # ------------------------------\n",
    "        print(f\"[EPOCH {epoch}] Running GridSearchCV (cv={CV_FOLDS}) ...\")\n",
    "        grid = GridSearchCV(\n",
    "            SVC(),\n",
    "            GRID_PARAM_GRID,\n",
    "            cv=CV_FOLDS,\n",
    "            scoring=SCORING,\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        grid.fit(X_train, y_train)\n",
    "        best_model = grid.best_estimator_\n",
    "        best_params_overall = grid.best_params_\n",
    "        print(f\"[EPOCH {epoch}] Best params: {best_params_overall}  CV {SCORING}={grid.best_score_:.4f}\")\n",
    "\n",
    "        # Fit best model\n",
    "        best_model.fit(X_train, y_train)\n",
    "\n",
    "        # Validation\n",
    "        if val_X_scaled is not None:\n",
    "            preds_val = best_model.predict(val_X_scaled)\n",
    "            acc_val = accuracy_score(val_y, preds_val)\n",
    "            f1_val = f1_score(val_y, preds_val, average=\"macro\")\n",
    "            print(f\"[EPOCH {epoch}] Validation: Acc={acc_val:.4f}  F1_macro={f1_val:.4f}\")\n",
    "\n",
    "        # Test\n",
    "        if test_X_scaled is not None:\n",
    "            preds_test = best_model.predict(test_X_scaled)\n",
    "            acc_test = accuracy_score(test_y, preds_test)\n",
    "            f1_test = f1_score(test_y, preds_test, average=\"macro\")\n",
    "            print(f\"[EPOCH {epoch}] Test: Acc={acc_test:.4f}  F1_macro={f1_test:.4f}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Final evaluation\n",
    "    # ------------------------------\n",
    "    final_model = best_model\n",
    "    if final_model is not None and test_X is not None:\n",
    "        preds = final_model.predict(test_X_scaled)\n",
    "        acc = accuracy_score(test_y, preds)\n",
    "        f1 = f1_score(test_y, preds, average=\"macro\")\n",
    "        print(\"\\n=== FINAL EVALUATION ON TEST POOL ===\")\n",
    "        print(f\"Test Acc={acc:.4f}  F1_macro={f1:.4f}\")\n",
    "        print(classification_report(test_y, preds, target_names=[\"Secure(False)\", \"Attack(True)\"]))\n",
    "    else:\n",
    "        print(\"No final model or test pool to evaluate.\")\n",
    "\n",
    "    return final_model, feature_names_master, best_params_overall\n",
    "\n",
    "# ------------------------------\n",
    "# Main\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    print(f\"[START] Training RBF/Linear SVM with GridSearchCV (epochs={EPOCHS})...\")\n",
    "    try:\n",
    "        svm_model, features, best_params = train_svm_rbf_grid(feature_files, base_path, epochs=EPOCHS)\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Training aborted: {e}\")\n",
    "        svm_model, features, best_params = None, None, None\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"[DONE] Total time: {elapsed:.1f}s\")\n",
    "    if best_params is not None:\n",
    "        print(f\"[RESULT] Best params (last epoch): {best_params}\")\n",
    "\n",
    "    print(\"\\n[START CLEANUP]\")\n",
    "    tmpdir = tempfile.gettempdir()\n",
    "    for pattern in [\"*.tmp\", \"*.temp\", \"tmp*\"]:\n",
    "        for f in glob.glob(os.path.join(tmpdir, pattern)):\n",
    "            safe_remove(f)\n",
    "    print(\"[CLEANUP] Completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seawsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
