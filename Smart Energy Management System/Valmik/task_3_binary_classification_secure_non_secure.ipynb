{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9974698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mqtt_ids_svm_train.py\n",
    "# Batch + epoch training of a nonlinear soft-margin SVM (via SGD hinge loss approximation)\n",
    "# for binary classification: secure (False) vs under attack (True).\n",
    "#\n",
    "# Dataset files are read in chunks to support large data.\n",
    "# The target column \"attack\" is True if under attack, False if secure.\n",
    "# Validation and test pools are sampled and stored in-memory.\n",
    "# Cleanup removes intermediate files created during preprocessing.\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import tempfile\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import random\n",
    "\n",
    "# ------------------------------\n",
    "# Config\n",
    "# ------------------------------\n",
    "base_path = \"./\"\n",
    "\n",
    "folders = {\n",
    "    \"packet\": \"packet_features\",\n",
    "    \"uniflow\": \"uniflow_features\",\n",
    "    \"biflow\": \"biflow_features\"\n",
    "}\n",
    "\n",
    "files = {\n",
    "    \"normal\": \"normal.csv\",\n",
    "    \"sparta\": \"sparta.csv\",\n",
    "    \"scan_A\": \"scan_A.csv\",\n",
    "    \"mqtt_bruteforce\": \"mqtt_bruteforce.csv\",\n",
    "    \"scan_sU\": \"scan_sU.csv\"\n",
    "}\n",
    "\n",
    "def build_filenames(prefix):\n",
    "    return {\n",
    "        \"normal\": f\"{prefix}_normal.csv\",\n",
    "        \"sparta\": f\"{prefix}_sparta.csv\",\n",
    "        \"scan_A\": f\"{prefix}_scan_A.csv\",\n",
    "        \"mqtt_bruteforce\": f\"{prefix}_mqtt_bruteforce.csv\",\n",
    "        \"scan_sU\": f\"{prefix}_scan_sU.csv\"\n",
    "    }\n",
    "\n",
    "feature_files = {\n",
    "    \"packet\": files,\n",
    "    \"uniflow\": build_filenames(\"uniflow\"),\n",
    "    \"biflow\": build_filenames(\"biflow\")\n",
    "}\n",
    "\n",
    "CHUNKSIZE = 200000\n",
    "TRAIN_FRACTION = 0.80\n",
    "VAL_FRACTION = 0.10\n",
    "TEST_FRACTION = 0.10\n",
    "\n",
    "EPOCHS = 3\n",
    "SAMPLE_VAL_MAX = 20000\n",
    "SAMPLE_TEST_MAX = 20000\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# ------------------------------\n",
    "# Cleanup helpers\n",
    "# ------------------------------\n",
    "TEMP_FILES_CREATED = []\n",
    "\n",
    "def safe_remove(path):\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)\n",
    "            print(f\"[CLEANUP] Removed file: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[CLEANUP] Could not remove {path}: {e}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Data streaming\n",
    "# ------------------------------\n",
    "def stream_chunks(feature_files_map, base_path, chunksize=CHUNKSIZE):\n",
    "    \"\"\"Yield (level, file_key, filepath, chunk_df).\"\"\"\n",
    "    for level, file_dict in feature_files_map.items():\n",
    "        folder_path = os.path.join(base_path, folders[level])\n",
    "        for key, fname in file_dict.items():\n",
    "            fpath = os.path.join(folder_path, fname)\n",
    "            if not os.path.isfile(fpath):\n",
    "                print(f\"[WARN] Missing: {fpath}\")\n",
    "                continue\n",
    "            try:\n",
    "                for chunk in pd.read_csv(fpath, chunksize=chunksize, low_memory=False):\n",
    "                    yield (level, key, fpath, chunk)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to read {fpath}: {e}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Preprocess chunk\n",
    "# ------------------------------\n",
    "def preprocess_chunk(df, file_key, expected_features=None):\n",
    "    df = df.copy()\n",
    "    # Boolean target: False = normal, True = attack\n",
    "    df[\"attack\"] = (file_key != \"normal\")\n",
    "\n",
    "    y = df[\"attack\"].astype(int).values\n",
    "    numeric_df = df.select_dtypes(include=[np.number]).copy()\n",
    "    if \"attack\" in numeric_df.columns:\n",
    "        numeric_df = numeric_df.drop(columns=[\"attack\"])\n",
    "    if \"label\" in numeric_df.columns:\n",
    "        numeric_df = numeric_df.drop(columns=[\"label\"])\n",
    "\n",
    "    if expected_features is not None:\n",
    "        common = [c for c in expected_features if c in numeric_df.columns]\n",
    "        numeric_df = numeric_df[common]\n",
    "        feature_names = common\n",
    "    else:\n",
    "        feature_names = list(numeric_df.columns)\n",
    "\n",
    "    X = numeric_df.fillna(0.0).values.astype(np.float32)\n",
    "    return X, y, feature_names\n",
    "\n",
    "# ------------------------------\n",
    "# Training loop\n",
    "# ------------------------------\n",
    "def train_svm_batchwise(feature_files_map, base_path, epochs=EPOCHS):\n",
    "    scaler = StandardScaler(with_mean=False)  # sparse-friendly\n",
    "    svm = SGDClassifier(\n",
    "        loss=\"hinge\",     # SVM hinge loss\n",
    "        penalty=\"l2\",     # soft-margin\n",
    "        alpha=0.0001,     # regularization strength\n",
    "        max_iter=1,\n",
    "        learning_rate=\"optimal\",\n",
    "        tol=None,\n",
    "        warm_start=True\n",
    "    )\n",
    "\n",
    "    feature_names_master = None\n",
    "    val_X, val_y = None, None\n",
    "    test_X, test_y = None, None\n",
    "    total_batches = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\n=== EPOCH {epoch}/{epochs} ===\")\n",
    "        for level, file_key, filepath, chunk in stream_chunks(feature_files_map, base_path):\n",
    "            total_batches += 1\n",
    "            tag = f\"ep{epoch}_b{total_batches}\"\n",
    "            print(f\"[BATCH {tag}] {level}/{file_key} shape={chunk.shape}\")\n",
    "\n",
    "            try:\n",
    "                if feature_names_master is None:\n",
    "                    X, y, features = preprocess_chunk(chunk, file_key)\n",
    "                    feature_names_master = features\n",
    "                else:\n",
    "                    X, y, _ = preprocess_chunk(chunk, file_key, expected_features=feature_names_master)\n",
    "            except Exception as e:\n",
    "                print(f\"[SKIP] {e}\")\n",
    "                continue\n",
    "\n",
    "            n = X.shape[0]\n",
    "            if n == 0: continue\n",
    "            rnd = np.random.rand(n)\n",
    "            train_mask = rnd < TRAIN_FRACTION\n",
    "            val_mask = (rnd >= TRAIN_FRACTION) & (rnd < TRAIN_FRACTION + VAL_FRACTION)\n",
    "            test_mask = rnd >= TRAIN_FRACTION + VAL_FRACTION\n",
    "\n",
    "            X_train, y_train = X[train_mask], y[train_mask]\n",
    "            X_val, y_val = X[val_mask], y[val_mask]\n",
    "            X_test, y_test_ = X[test_mask], y[test_mask]\n",
    "\n",
    "            # Scale + partial fit\n",
    "            if X_train.shape[0] > 0:\n",
    "                X_train = scaler.partial_fit(X_train).transform(X_train)\n",
    "                if not hasattr(svm, \"classes_\"):\n",
    "                    svm.partial_fit(X_train, y_train, classes=np.array([0, 1]))\n",
    "                else:\n",
    "                    svm.partial_fit(X_train, y_train)\n",
    "                print(f\"[BATCH {tag}] trained on {X_train.shape[0]} rows.\")\n",
    "\n",
    "            # Validation pool\n",
    "            if X_val.shape[0] > 0:\n",
    "                X_val = scaler.transform(X_val)\n",
    "                if val_X is None:\n",
    "                    take = min(X_val.shape[0], SAMPLE_VAL_MAX)\n",
    "                    idxs = np.random.choice(X_val.shape[0], take, replace=False)\n",
    "                    val_X, val_y = X_val[idxs], y_val[idxs]\n",
    "                else:\n",
    "                    val_X = np.vstack([val_X, X_val])\n",
    "                    val_y = np.concatenate([val_y, y_val])\n",
    "                    if val_X.shape[0] > SAMPLE_VAL_MAX:\n",
    "                        idxs = np.random.choice(val_X.shape[0], SAMPLE_VAL_MAX, replace=False)\n",
    "                        val_X, val_y = val_X[idxs], val_y[idxs]\n",
    "\n",
    "            # Test pool\n",
    "            if X_test.shape[0] > 0:\n",
    "                X_test = scaler.transform(X_test)\n",
    "                if test_X is None:\n",
    "                    take = min(X_test.shape[0], SAMPLE_TEST_MAX)\n",
    "                    idxs = np.random.choice(X_test.shape[0], take, replace=False)\n",
    "                    test_X, test_y = X_test[idxs], y_test_[idxs]\n",
    "                else:\n",
    "                    test_X = np.vstack([test_X, X_test])\n",
    "                    test_y = np.concatenate([test_y, y_test_])\n",
    "                    if test_X.shape[0] > SAMPLE_TEST_MAX:\n",
    "                        idxs = np.random.choice(test_X.shape[0], SAMPLE_TEST_MAX, replace=False)\n",
    "                        test_X, test_y = test_X[idxs], test_y[idxs]\n",
    "\n",
    "        # Epoch validation\n",
    "        if val_X is not None and val_X.shape[0] > 0:\n",
    "            preds = svm.predict(val_X)\n",
    "            acc = accuracy_score(val_y, preds)\n",
    "            f1 = f1_score(val_y, preds, average=\"macro\")\n",
    "            print(f\"[EPOCH {epoch}] Val Acc={acc:.4f}  F1={f1:.4f}\")\n",
    "\n",
    "    # Final test\n",
    "    if test_X is not None and test_X.shape[0] > 0:\n",
    "        preds = svm.predict(test_X)\n",
    "        acc = accuracy_score(test_y, preds)\n",
    "        f1 = f1_score(test_y, preds, average=\"macro\")\n",
    "        print(\"\\n=== FINAL TEST METRICS ===\")\n",
    "        print(f\"Test Acc={acc:.4f}  F1={f1:.4f}\")\n",
    "        print(classification_report(test_y, preds, target_names=[\"Secure(False)\", \"Attack(True)\"]))\n",
    "    else:\n",
    "        print(\"No test data available.\")\n",
    "\n",
    "    return svm, feature_names_master\n",
    "\n",
    "# ------------------------------\n",
    "# Main\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    start = time.time()\n",
    "    print(\"[START] Training nonlinear SVM with batch/epoch approach...\")\n",
    "    try:\n",
    "        svm, features = train_svm_batchwise(feature_files, base_path, epochs=EPOCHS)\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Error: {e}\")\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"[DONE] Total time {elapsed:.1f}s\")\n",
    "\n",
    "    # Cleanup temp files\n",
    "    print(\"\\n[START CLEANUP]\")\n",
    "    tmpdir = tempfile.gettempdir()\n",
    "    for pattern in [\"*.tmp\", \"*.temp\", \"tmp*\"]:\n",
    "        for f in glob.glob(os.path.join(tmpdir, pattern)):\n",
    "            safe_remove(f)\n",
    "    print(\"[CLEANUP] Completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seawsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
