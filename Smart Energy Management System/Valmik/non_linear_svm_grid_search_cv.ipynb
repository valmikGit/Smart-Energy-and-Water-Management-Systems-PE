{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59d3cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mqtt_ids_svm_train_rbf.py\n",
    "# Full batch training of nonlinear RBF SVM with GridSearchCV\n",
    "# for binary classification: secure (False) vs under attack (True).\n",
    "#\n",
    "# Dataset files are read in chunks to support large data.\n",
    "# The target column \"attack\" is True if under attack, False if secure.\n",
    "# Validation and test pools are sampled and stored in-memory.\n",
    "# Cleanup removes intermediate files created during preprocessing.\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import random\n",
    "\n",
    "# ------------------------------\n",
    "# Config\n",
    "# ------------------------------\n",
    "base_path = \"./\"\n",
    "\n",
    "folders = {\n",
    "    \"packet\": \"packet_features\",\n",
    "    \"uniflow\": \"uniflow_features\",\n",
    "    \"biflow\": \"biflow_features\"\n",
    "}\n",
    "\n",
    "files = {\n",
    "    \"normal\": \"normal.csv\",\n",
    "    \"sparta\": \"sparta.csv\",\n",
    "    \"scan_A\": \"scan_A.csv\",\n",
    "    \"mqtt_bruteforce\": \"mqtt_bruteforce.csv\",\n",
    "    \"scan_sU\": \"scan_sU.csv\"\n",
    "}\n",
    "\n",
    "def build_filenames(prefix):\n",
    "    return {\n",
    "        \"normal\": f\"{prefix}_normal.csv\",\n",
    "        \"sparta\": f\"{prefix}_sparta.csv\",\n",
    "        \"scan_A\": f\"{prefix}_scan_A.csv\",\n",
    "        \"mqtt_bruteforce\": f\"{prefix}_mqtt_bruteforce.csv\",\n",
    "        \"scan_sU\": f\"{prefix}_scan_sU.csv\"\n",
    "    }\n",
    "\n",
    "feature_files = {\n",
    "    \"packet\": files,\n",
    "    \"uniflow\": build_filenames(\"uniflow\"),\n",
    "    \"biflow\": build_filenames(\"biflow\")\n",
    "}\n",
    "\n",
    "CHUNKSIZE = 200000\n",
    "TRAIN_FRACTION = 0.80\n",
    "VAL_FRACTION = 0.10\n",
    "TEST_FRACTION = 0.10\n",
    "\n",
    "SAMPLE_VAL_MAX = 20000\n",
    "SAMPLE_TEST_MAX = 20000\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# ------------------------------\n",
    "# Cleanup helpers\n",
    "# ------------------------------\n",
    "def safe_remove(path):\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)\n",
    "            print(f\"[CLEANUP] Removed file: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[CLEANUP] Could not remove {path}: {e}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Data streaming\n",
    "# ------------------------------\n",
    "def stream_chunks(feature_files_map, base_path, chunksize=CHUNKSIZE):\n",
    "    \"\"\"Yield (level, file_key, filepath, chunk_df).\"\"\"\n",
    "    for level, file_dict in feature_files_map.items():\n",
    "        folder_path = os.path.join(base_path, folders[level])\n",
    "        for key, fname in file_dict.items():\n",
    "            fpath = os.path.join(folder_path, fname)\n",
    "            if not os.path.isfile(fpath):\n",
    "                print(f\"[WARN] Missing: {fpath}\")\n",
    "                continue\n",
    "            try:\n",
    "                for chunk in pd.read_csv(fpath, chunksize=chunksize, low_memory=False):\n",
    "                    yield (level, key, fpath, chunk)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to read {fpath}: {e}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Preprocess chunk\n",
    "# ------------------------------\n",
    "def preprocess_chunk(df, file_key, expected_features=None):\n",
    "    df = df.copy()\n",
    "    # Boolean target: False = normal, True = attack\n",
    "    df[\"attack\"] = (file_key != \"normal\")\n",
    "\n",
    "    y = df[\"attack\"].astype(int).values\n",
    "    numeric_df = df.select_dtypes(include=[np.number]).copy()\n",
    "    numeric_df = numeric_df.drop(columns=[c for c in [\"attack\", \"label\"] if c in numeric_df.columns])\n",
    "\n",
    "    if expected_features is not None:\n",
    "        # Reindex to master feature set, fill missing with 0\n",
    "        numeric_df = numeric_df.reindex(columns=expected_features, fill_value=0.0)\n",
    "        feature_names = expected_features\n",
    "    else:\n",
    "        feature_names = list(numeric_df.columns)\n",
    "\n",
    "    X = numeric_df.fillna(0.0).values.astype(np.float32)\n",
    "    return X, y, feature_names\n",
    "\n",
    "# ------------------------------\n",
    "# Training with RBF SVM + GridSearch\n",
    "# ------------------------------\n",
    "def train_svm_rbf(feature_files_map, base_path):\n",
    "    scaler = StandardScaler(with_mean=False)  # sparse-friendly\n",
    "\n",
    "    feature_names_master = None\n",
    "    train_X, train_y = [], []\n",
    "    val_X, val_y = None, None\n",
    "    test_X, test_y = None, None\n",
    "\n",
    "    print(\"\\n=== Collecting data ===\")\n",
    "    for level, file_key, filepath, chunk in stream_chunks(feature_files_map, base_path):\n",
    "        print(f\"[CHUNK] {level}/{file_key} shape={chunk.shape}\")\n",
    "        try:\n",
    "            if feature_names_master is None:\n",
    "                X, y, features = preprocess_chunk(chunk, file_key)\n",
    "                feature_names_master = features\n",
    "            else:\n",
    "                X, y, _ = preprocess_chunk(chunk, file_key, expected_features=feature_names_master)\n",
    "        except Exception as e:\n",
    "            print(f\"[SKIP] {e}\")\n",
    "            continue\n",
    "\n",
    "        if X.shape[1] == 0 or X.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        rnd = np.random.rand(X.shape[0])\n",
    "        train_mask = rnd < TRAIN_FRACTION\n",
    "        val_mask = (rnd >= TRAIN_FRACTION) & (rnd < TRAIN_FRACTION + VAL_FRACTION)\n",
    "        test_mask = rnd >= TRAIN_FRACTION + VAL_FRACTION\n",
    "\n",
    "        train_X.append(X[train_mask])\n",
    "        train_y.append(y[train_mask])\n",
    "\n",
    "        # Validation pool\n",
    "        if np.any(val_mask):\n",
    "            X_val, y_val = X[val_mask], y[val_mask]\n",
    "            if val_X is None:\n",
    "                take = min(X_val.shape[0], SAMPLE_VAL_MAX)\n",
    "                idxs = np.random.choice(X_val.shape[0], take, replace=False)\n",
    "                val_X, val_y = X_val[idxs], y_val[idxs]\n",
    "            else:\n",
    "                val_X = np.vstack([val_X, X_val])\n",
    "                val_y = np.concatenate([val_y, y_val])\n",
    "                if val_X.shape[0] > SAMPLE_VAL_MAX:\n",
    "                    idxs = np.random.choice(val_X.shape[0], SAMPLE_VAL_MAX, replace=False)\n",
    "                    val_X, val_y = val_X[idxs], val_y[idxs]\n",
    "\n",
    "        # Test pool\n",
    "        if np.any(test_mask):\n",
    "            X_test, y_test_ = X[test_mask], y[test_mask]\n",
    "            if test_X is None:\n",
    "                take = min(X_test.shape[0], SAMPLE_TEST_MAX)\n",
    "                idxs = np.random.choice(X_test.shape[0], take, replace=False)\n",
    "                test_X, test_y = X_test[idxs], y_test_[idxs]\n",
    "            else:\n",
    "                test_X = np.vstack([test_X, X_test])\n",
    "                test_y = np.concatenate([test_y, y_test_])\n",
    "                if test_X.shape[0] > SAMPLE_TEST_MAX:\n",
    "                    idxs = np.random.choice(test_X.shape[0], SAMPLE_TEST_MAX, replace=False)\n",
    "                    test_X, test_y = test_X[idxs], test_y[idxs]\n",
    "\n",
    "    # Combine all training chunks\n",
    "    train_X = np.vstack(train_X)\n",
    "    train_y = np.concatenate(train_y)\n",
    "\n",
    "    # Scale\n",
    "    train_X = scaler.fit_transform(train_X)\n",
    "    if val_X is not None:\n",
    "        val_X = scaler.transform(val_X)\n",
    "    if test_X is not None:\n",
    "        test_X = scaler.transform(test_X)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Grid Search with RBF SVM\n",
    "    # ------------------------------\n",
    "    print(\"\\n=== Training RBF SVM with GridSearchCV ===\")\n",
    "    param_grid = {\n",
    "        \"C\": [0.1, 1, 10],\n",
    "        \"gamma\": [\"scale\", 0.01, 0.001],\n",
    "        \"kernel\": [\"rbf\"]\n",
    "    }\n",
    "    grid = GridSearchCV(SVC(), param_grid, cv=3, scoring=\"f1_macro\", verbose=2, n_jobs=-1)\n",
    "    grid.fit(train_X, train_y)\n",
    "\n",
    "    print(f\"\\n[GRID SEARCH] Best Params: {grid.best_params_}\")\n",
    "    print(f\"[GRID SEARCH] Best CV Score (F1 macro): {grid.best_score_:.4f}\")\n",
    "\n",
    "    best_svm = grid.best_estimator_\n",
    "\n",
    "    # Validation metrics\n",
    "    if val_X is not None:\n",
    "        preds = best_svm.predict(val_X)\n",
    "        acc = accuracy_score(val_y, preds)\n",
    "        f1 = f1_score(val_y, preds, average=\"macro\")\n",
    "        print(f\"\\n=== VALIDATION METRICS ===\")\n",
    "        print(f\"Val Acc={acc:.4f}  F1={f1:.4f}\")\n",
    "\n",
    "    # Test metrics\n",
    "    if test_X is not None:\n",
    "        preds = best_svm.predict(test_X)\n",
    "        acc = accuracy_score(test_y, preds)\n",
    "        f1 = f1_score(test_y, preds, average=\"macro\")\n",
    "        print(\"\\n=== FINAL TEST METRICS ===\")\n",
    "        print(f\"Test Acc={acc:.4f}  F1={f1:.4f}\")\n",
    "        print(classification_report(test_y, preds, target_names=[\"Secure(False)\", \"Attack(True)\"]))\n",
    "    else:\n",
    "        print(\"No test data available.\")\n",
    "\n",
    "    return best_svm, feature_names_master\n",
    "\n",
    "# ------------------------------\n",
    "# Main\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    start = time.time()\n",
    "    print(\"[START] Training nonlinear RBF SVM with GridSearchCV...\")\n",
    "    try:\n",
    "        svm, features = train_svm_rbf(feature_files, base_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Error: {e}\")\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"[DONE] Total time {elapsed:.1f}s\")\n",
    "\n",
    "    # Cleanup temp files\n",
    "    print(\"\\n[START CLEANUP]\")\n",
    "    tmpdir = tempfile.gettempdir()\n",
    "    for pattern in [\"*.tmp\", \"*.temp\", \"tmp*\"]:\n",
    "        for f in glob.glob(os.path.join(tmpdir, pattern)):\n",
    "            safe_remove(f)\n",
    "    print(\"[CLEANUP] Completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seawsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
