{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cad1a23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Start: Full scatter plot (attack vs no-attack) ==\n",
      "[DISCOVERED] Numeric columns (sample): ['bwd_max_iat', 'bwd_max_pkt_len', 'bwd_mean_iat', 'bwd_mean_pkt_len', 'bwd_min_iat', 'bwd_min_pkt_len', 'bwd_num_bytes', 'bwd_num_pkts', 'bwd_num_psh_flags', 'bwd_num_rst_flags', 'bwd_num_urg_flags', 'bwd_std_iat', 'bwd_std_pkt_len', 'dst_port', 'fwd_max_iat', 'fwd_max_pkt_len', 'fwd_mean_iat', 'fwd_mean_pkt_len', 'fwd_min_iat', 'fwd_min_pkt_len'] (total 69)\n",
      "[SELECTED] Random numeric columns chosen for plotting: X='fwd_max_iat', Y='bwd_mean_pkt_len'\n",
      "[WARN] Collected 15099994 points; exceeding safe accumulation multiple. Stopping collection early.\n",
      "[COLLECT] Collected 15099994 points in total across files.\n",
      "[DOWNSAMPLE] Downsampling from 15099994 to 3000000 points for safety.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Valmik Belgaonkar\\AppData\\Local\\Temp\\ipykernel_11152\\855460574.py:203: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\Valmik Belgaonkar\\AppData\\Local\\Temp\\ipykernel_11152\\855460574.py:204: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.savefig(out_path, dpi=200)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVED] Scatter plot saved to: outputs_scatter\\combined_scatter.png\n",
      "[CLEANUP] Could not remove C:\\Users\\VALMIK~1\\AppData\\Local\\Temp\\0bfaf4d2-a756-4c46-8b84-425ea23f954d.tmp: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\VALMIK~1\\\\AppData\\\\Local\\\\Temp\\\\0bfaf4d2-a756-4c46-8b84-425ea23f954d.tmp'\n",
      "[CLEANUP] Could not remove C:\\Users\\VALMIK~1\\AppData\\Local\\Temp\\0fcf55a5-762e-4589-b10b-5eca2bef0e65.tmp: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\VALMIK~1\\\\AppData\\\\Local\\\\Temp\\\\0fcf55a5-762e-4589-b10b-5eca2bef0e65.tmp'\n",
      "[CLEANUP] Could not remove C:\\Users\\VALMIK~1\\AppData\\Local\\Temp\\30804d63-b694-4472-8339-72895e4ea950.tmp: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\VALMIK~1\\\\AppData\\\\Local\\\\Temp\\\\30804d63-b694-4472-8339-72895e4ea950.tmp'\n",
      "[CLEANUP] Could not remove C:\\Users\\VALMIK~1\\AppData\\Local\\Temp\\4667ddf0-54d7-40fd-bc27-44bd070e70bb.tmp: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\VALMIK~1\\\\AppData\\\\Local\\\\Temp\\\\4667ddf0-54d7-40fd-bc27-44bd070e70bb.tmp'\n",
      "[CLEANUP] Could not remove C:\\Users\\VALMIK~1\\AppData\\Local\\Temp\\88e909b0-5e37-4d44-9f80-a82e77c7816a.tmp: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\VALMIK~1\\\\AppData\\\\Local\\\\Temp\\\\88e909b0-5e37-4d44-9f80-a82e77c7816a.tmp'\n",
      "[CLEANUP] Could not remove C:\\Users\\VALMIK~1\\AppData\\Local\\Temp\\94658116-215d-4f29-88c7-3f32cf28af66.tmp: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\VALMIK~1\\\\AppData\\\\Local\\\\Temp\\\\94658116-215d-4f29-88c7-3f32cf28af66.tmp'\n",
      "[CLEANUP] Could not remove C:\\Users\\VALMIK~1\\AppData\\Local\\Temp\\a0d3f020-7c37-42d8-a7bf-eec8f1898bd6.tmp: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\VALMIK~1\\\\AppData\\\\Local\\\\Temp\\\\a0d3f020-7c37-42d8-a7bf-eec8f1898bd6.tmp'\n",
      "[CLEANUP] Could not remove C:\\Users\\VALMIK~1\\AppData\\Local\\Temp\\addd2704-d8f4-471b-b38a-4274deb71f46.tmp: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\VALMIK~1\\\\AppData\\\\Local\\\\Temp\\\\addd2704-d8f4-471b-b38a-4274deb71f46.tmp'\n",
      "[CLEANUP] Could not remove C:\\Users\\VALMIK~1\\AppData\\Local\\Temp\\af77b8b8-b28a-4a8c-8fa4-fbe582229b6f.tmp: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\VALMIK~1\\\\AppData\\\\Local\\\\Temp\\\\af77b8b8-b28a-4a8c-8fa4-fbe582229b6f.tmp'\n",
      "[CLEANUP] Could not remove C:\\Users\\VALMIK~1\\AppData\\Local\\Temp\\b774170b-4c60-49e6-99d0-b05806e56175.tmp: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\VALMIK~1\\\\AppData\\\\Local\\\\Temp\\\\b774170b-4c60-49e6-99d0-b05806e56175.tmp'\n",
      "[CLEANUP] Could not remove C:\\Users\\VALMIK~1\\AppData\\Local\\Temp\\bc0c605e-8d25-42fa-a8fd-fa65d61701ff.tmp: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\VALMIK~1\\\\AppData\\\\Local\\\\Temp\\\\bc0c605e-8d25-42fa-a8fd-fa65d61701ff.tmp'\n",
      "[CLEANUP] Could not remove C:\\Users\\VALMIK~1\\AppData\\Local\\Temp\\c77a159d-d9bc-43be-8239-4facab1f12dd.tmp: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\VALMIK~1\\\\AppData\\\\Local\\\\Temp\\\\c77a159d-d9bc-43be-8239-4facab1f12dd.tmp'\n",
      "[CLEANUP] Could not remove C:\\Users\\VALMIK~1\\AppData\\Local\\Temp\\cd318abe-0d64-4648-b60f-478517c542d0.tmp: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\VALMIK~1\\\\AppData\\\\Local\\\\Temp\\\\cd318abe-0d64-4648-b60f-478517c542d0.tmp'\n",
      "[CLEANUP] Could not remove C:\\Users\\VALMIK~1\\AppData\\Local\\Temp\\ed14c6f3-a0ce-479b-a649-bdd12a98b57a.tmp: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\VALMIK~1\\\\AppData\\\\Local\\\\Temp\\\\ed14c6f3-a0ce-479b-a649-bdd12a98b57a.tmp'\n",
      "[CLEANUP] Removed temp file: C:\\Users\\VALMIK~1\\AppData\\Local\\Temp\\wct1D73.tmp\n",
      "[CLEANUP] Removed temp file: C:\\Users\\VALMIK~1\\AppData\\Local\\Temp\\wct6474.tmp\n",
      "[CLEANUP] Removed temp file: C:\\Users\\VALMIK~1\\AppData\\Local\\Temp\\wctAAE1.tmp\n",
      "[CLEANUP] Removed temp file: C:\\Users\\VALMIK~1\\AppData\\Local\\Temp\\wctD77D.tmp\n",
      "[CLEANUP] Removed temp file: C:\\Users\\VALMIK~1\\AppData\\Local\\Temp\\wctED7A.tmp\n",
      "[CLEANUP] Removed 5 temp files.\n",
      "== Done ==\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "full_scatter_attack_vs_noattack.py\n",
    "\n",
    "Create a scatter plot using 2 randomly chosen numeric columns (from the dataset)\n",
    "where point color is determined by attack (1) vs no-attack (0). The script:\n",
    "\n",
    " - Scans available CSV feature files to discover numeric columns (first chunk of each file).\n",
    " - Randomly selects two numeric columns (seeded for reproducibility).\n",
    " - Streams every CSV chunk, extracts the chosen columns and the attack label per-row\n",
    "   (label: 0 for \"normal\" file, 1 for others) and collects all rows.\n",
    " - If the total number of points becomes extremely large, the script will downsample\n",
    "   to a SAFETY_MAX number of points to avoid running out of memory / crashing.\n",
    " - Produces a scatter plot saved to OUTPUT_DIR/combined_scatter.png.\n",
    " - Cleans up common temporary files (*.tmp, *.temp) in system temp dir and base_path.\n",
    "\n",
    "NOTE:\n",
    " - This script attempts to plot *all* data points, but includes a safety cap to avoid\n",
    "   OOM in practical environments. Adjust SAFETY_MAX if you have sufficient memory.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import tempfile\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------\n",
    "# Config - adjust as needed\n",
    "# ------------------------------\n",
    "base_path = \"./\"   # root folder where feature folders live\n",
    "folders = {\n",
    "    \"packet\": \"packet_features\",\n",
    "    \"uniflow\": \"uniflow_features\",\n",
    "    \"biflow\": \"biflow_features\"\n",
    "}\n",
    "files = {\n",
    "    \"normal\": \"normal.csv\",\n",
    "    \"sparta\": \"sparta.csv\",\n",
    "    \"scan_A\": \"scan_A.csv\",\n",
    "    \"mqtt_bruteforce\": \"mqtt_bruteforce.csv\",\n",
    "    \"scan_sU\": \"scan_sU.csv\"\n",
    "}\n",
    "def build_filenames(prefix):\n",
    "    return {\n",
    "        \"normal\": f\"{prefix}_normal.csv\",\n",
    "        \"sparta\": f\"{prefix}_sparta.csv\",\n",
    "        \"scan_A\": f\"{prefix}_scan_A.csv\",\n",
    "        \"mqtt_bruteforce\": f\"{prefix}_mqtt_bruteforce.csv\",\n",
    "        \"scan_sU\": f\"{prefix}_scan_sU.csv\"\n",
    "    }\n",
    "\n",
    "feature_files = {\n",
    "    \"packet\": files,\n",
    "    \"uniflow\": build_filenames(\"uniflow\"),\n",
    "    \"biflow\": build_filenames(\"biflow\")\n",
    "}\n",
    "\n",
    "OUTPUT_DIR = \"outputs_scatter\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "CHUNK_SIZE = 100000       # pandas read_csv chunksize for streaming\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Safety cap: if there are more than SAFETY_MAX points, we will downsample to SAFETY_MAX before plotting.\n",
    "# Set to None to attempt plotting all (dangerous if dataset is huge).\n",
    "SAFETY_MAX = 3_000_000\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# ------------------------------\n",
    "# Helpers\n",
    "# ------------------------------\n",
    "def discover_numeric_columns(feature_files_map, base_path, chunksize=CHUNK_SIZE, max_files_first_chunks=50):\n",
    "    \"\"\"\n",
    "    Inspect the first chunk of each CSV file (or until enough inspected) and build\n",
    "    the union of numeric columns observed across files. Return a sorted list.\n",
    "    \"\"\"\n",
    "    numeric_cols_set = set()\n",
    "    files_inspected = 0\n",
    "\n",
    "    for level, file_dict in feature_files_map.items():\n",
    "        folder_path = os.path.join(base_path, folders[level])\n",
    "        for name, fname in file_dict.items():\n",
    "            fpath = os.path.join(folder_path, fname)\n",
    "            if not os.path.exists(fpath):\n",
    "                continue\n",
    "            try:\n",
    "                # Read only the first chunk from this file\n",
    "                for chunk in pd.read_csv(fpath, chunksize=chunksize, low_memory=False):\n",
    "                    nc = chunk.select_dtypes(include=[np.number]).columns.tolist()\n",
    "                    # avoid adding obvious label columns that are not features\n",
    "                    nc = [c for c in nc if c.lower() not in (\"label\", \"attack\", \"attack_type\")]\n",
    "                    numeric_cols_set.update(nc)\n",
    "                    files_inspected += 1\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not read first chunk of {fpath}: {e}\")\n",
    "                continue\n",
    "            if files_inspected >= max_files_first_chunks:\n",
    "                break\n",
    "        if files_inspected >= max_files_first_chunks:\n",
    "            break\n",
    "\n",
    "    numeric_cols = sorted(list(numeric_cols_set))\n",
    "    return numeric_cols\n",
    "\n",
    "def pick_two_random_numeric_columns(numeric_cols):\n",
    "    \"\"\"Pick two distinct numeric columns randomly (seeded).\"\"\"\n",
    "    if len(numeric_cols) < 2:\n",
    "        return None, None\n",
    "    cols = random.sample(numeric_cols, 2)\n",
    "    return cols[0], cols[1]\n",
    "\n",
    "def stream_collect_all(feature_files_map, base_path, x_col, y_col, chunksize=CHUNK_SIZE, safety_max=SAFETY_MAX):\n",
    "    \"\"\"\n",
    "    Stream all CSVs chunk-by-chunk and collect the chosen x_col and y_col values and labels.\n",
    "    Returns arrays X (n x 2) and y (n,), possibly downsampled to safety_max.\n",
    "    \"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    labels = []\n",
    "    total = 0\n",
    "\n",
    "    for level, file_dict in feature_files_map.items():\n",
    "        folder_path = os.path.join(base_path, folders[level])\n",
    "        for name, fname in file_dict.items():\n",
    "            fpath = os.path.join(folder_path, fname)\n",
    "            if not os.path.exists(fpath):\n",
    "                continue\n",
    "            label_value = 0 if name == \"normal\" else 1\n",
    "            try:\n",
    "                for chunk in pd.read_csv(fpath, chunksize=chunksize, low_memory=False):\n",
    "                    # if the desired columns aren't in this chunk, try selecting first two numeric columns\n",
    "                    if x_col not in chunk.columns or y_col not in chunk.columns:\n",
    "                        numeric_cols = chunk.select_dtypes(include=[np.number]).columns.tolist()\n",
    "                        numeric_cols = [c for c in numeric_cols if c.lower() not in (\"label\", \"attack\", \"attack_type\")]\n",
    "                        if len(numeric_cols) >= 2:\n",
    "                            use_x, use_y = numeric_cols[0], numeric_cols[1]\n",
    "                        else:\n",
    "                            continue  # skip chunk if no usable numeric features\n",
    "                    else:\n",
    "                        use_x, use_y = x_col, y_col\n",
    "\n",
    "                    sub = chunk[[use_x, use_y]].copy()\n",
    "                    sub = sub.dropna()\n",
    "                    if sub.shape[0] == 0:\n",
    "                        continue\n",
    "\n",
    "                    # extend lists\n",
    "                    xs.extend(sub.iloc[:, 0].astype(np.float64).tolist())\n",
    "                    ys.extend(sub.iloc[:, 1].astype(np.float64).tolist())\n",
    "                    labels.extend([label_value] * len(sub))\n",
    "                    total += len(sub)\n",
    "\n",
    "                    # safety: if we've exceeded safety_max massively, we can stop early and downsample later\n",
    "                    if safety_max is not None and total > (safety_max * 5):\n",
    "                        # stop early to avoid running out of memory collecting an enormous buffer\n",
    "                        print(f\"[WARN] Collected {total} points; exceeding safe accumulation multiple. Stopping collection early.\")\n",
    "                        break\n",
    "                # if stop condition from inner loop triggered, break outer loops\n",
    "                if safety_max is not None and total > (safety_max * 5):\n",
    "                    break\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(f\"[WARN] {fpath} is empty; skipping.\")\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Error processing {fpath}: {e}\")\n",
    "        if safety_max is not None and total > (safety_max * 5):\n",
    "            break\n",
    "\n",
    "    if total == 0:\n",
    "        raise RuntimeError(\"No numeric data collected. Check CSVs and numeric columns.\")\n",
    "\n",
    "    X = np.column_stack((np.array(xs), np.array(ys)))\n",
    "    y = np.array(labels, dtype=np.int32)\n",
    "    print(f\"[COLLECT] Collected {X.shape[0]} points in total across files.\")\n",
    "\n",
    "    # Downsample to safety_max if needed\n",
    "    if safety_max is not None and X.shape[0] > safety_max:\n",
    "        print(f\"[DOWNSAMPLE] Downsampling from {X.shape[0]} to {safety_max} points for safety.\")\n",
    "        idxs = np.random.choice(X.shape[0], size=safety_max, replace=False)\n",
    "        X = X[idxs]\n",
    "        y = y[idxs]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def plot_scatter_full(X, y, x_label, y_label, out_path):\n",
    "    \"\"\"Plot scatter colored by label and save to out_path.\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    idx_no = (y == 0)\n",
    "    idx_yes = (y == 1)\n",
    "\n",
    "    # Plot no attack first (blue), attacks on top (red) for visibility\n",
    "    plt.scatter(X[idx_no, 0], X[idx_no, 1], c=\"tab:blue\", label=\"No Attack\", alpha=0.6, s=6)\n",
    "    plt.scatter(X[idx_yes, 0], X[idx_yes, 1], c=\"tab:red\", label=\"Attack\", alpha=0.6, s=6)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(f\"Scatter plot: {y_label} vs {x_label} (Attack=red, No Attack=blue)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"[SAVED] Scatter plot saved to: {out_path}\")\n",
    "\n",
    "def cleanup_temp_files(base_dirs=None):\n",
    "    \"\"\"Remove common temporary files from provided directories (tempdir + base_path by default).\"\"\"\n",
    "    if base_dirs is None:\n",
    "        base_dirs = [tempfile.gettempdir(), base_path]\n",
    "    patterns = [\"*.tmp\", \"*.temp\", \"tmp*\"]\n",
    "    removed = 0\n",
    "    for d in base_dirs:\n",
    "        for pat in patterns:\n",
    "            for f in glob.glob(os.path.join(d, pat)):\n",
    "                try:\n",
    "                    if os.path.isfile(f):\n",
    "                        os.remove(f)\n",
    "                        removed += 1\n",
    "                        print(f\"[CLEANUP] Removed temp file: {f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[CLEANUP] Could not remove {f}: {e}\")\n",
    "    if removed == 0:\n",
    "        print(\"[CLEANUP] No temp files found to remove.\")\n",
    "    else:\n",
    "        print(f\"[CLEANUP] Removed {removed} temp files.\")\n",
    "\n",
    "# ------------------------------\n",
    "# Main\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"== Start: Full scatter plot (attack vs no-attack) ==\")\n",
    "\n",
    "    # 1) Discover numeric columns across files\n",
    "    numeric_cols = discover_numeric_columns(feature_files, base_path, chunksize=CHUNK_SIZE)\n",
    "    if not numeric_cols or len(numeric_cols) < 2:\n",
    "        print(\"[ERROR] Not enough numeric columns discovered across dataset to make a scatter plot.\")\n",
    "        raise SystemExit(1)\n",
    "    print(f\"[DISCOVERED] Numeric columns (sample): {numeric_cols[:20]} (total {len(numeric_cols)})\")\n",
    "\n",
    "    # 2) Randomly pick two numeric columns\n",
    "    x_col, y_col = pick_two_random_numeric_columns(numeric_cols)\n",
    "    if x_col is None or y_col is None:\n",
    "        print(\"[ERROR] Unable to pick two numeric columns.\")\n",
    "        raise SystemExit(1)\n",
    "    print(f\"[SELECTED] Random numeric columns chosen for plotting: X='{x_col}', Y='{y_col}'\")\n",
    "\n",
    "    # 3) Stream and collect all rows for the two columns (with safety)\n",
    "    X_all, y_all = stream_collect_all(feature_files, base_path, x_col, y_col, chunksize=CHUNK_SIZE, safety_max=SAFETY_MAX)\n",
    "\n",
    "    # 4) Plot\n",
    "    out_file = os.path.join(OUTPUT_DIR, \"combined_scatter.png\")\n",
    "    plot_scatter_full(X_all, y_all, x_col, y_col, out_file)\n",
    "\n",
    "    # 5) Cleanup temporary files\n",
    "    cleanup_temp_files([tempfile.gettempdir(), base_path])\n",
    "\n",
    "    print(\"== Done ==\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seawsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
