{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd56540a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] scikit-optimize detected: using BayesSearchCV (GP surrogate).\n",
      "[START] Training RBF SVM with per-epoch Bayesian optimization (epochs=3)...\n",
      "\n",
      "=== EPOCH 1/3 ===\n",
      "[EPOCH 1] CHUNK packet/normal shape=(200000, 31)\n",
      "[INFO] Established master feature count: 27\n",
      "[EPOCH 1] CHUNK packet/normal shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/normal shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/normal shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/normal shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/normal shape=(56231, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/sparta shape=(76140, 31)\n",
      "[EPOCH 1] CHUNK packet/scan_A shape=(111392, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/mqtt_bruteforce shape=(45316, 31)\n",
      "[EPOCH 1] CHUNK packet/scan_sU shape=(200000, 31)\n",
      "[EPOCH 1] CHUNK packet/scan_sU shape=(33255, 31)\n",
      "[EPOCH 1] CHUNK uniflow/normal shape=(171836, 19)\n",
      "[EPOCH 1] CHUNK uniflow/sparta shape=(182407, 19)\n",
      "[EPOCH 1] CHUNK uniflow/scan_A shape=(51358, 19)\n",
      "[EPOCH 1] CHUNK uniflow/mqtt_bruteforce shape=(33079, 19)\n",
      "[EPOCH 1] CHUNK uniflow/scan_sU shape=(56845, 19)\n",
      "[EPOCH 1] CHUNK biflow/normal shape=(86008, 32)\n",
      "[EPOCH 1] CHUNK biflow/sparta shape=(91318, 32)\n",
      "[EPOCH 1] CHUNK biflow/scan_A shape=(25693, 32)\n",
      "[EPOCH 1] CHUNK biflow/mqtt_bruteforce shape=(16696, 32)\n",
      "[EPOCH 1] CHUNK biflow/scan_sU shape=(39664, 32)\n",
      "[INFO] Downsampled training set to 200000 samples.\n",
      "[EPOCH 1] Running BayesSearchCV (n_iter=25, cv=3) ...\n"
     ]
    }
   ],
   "source": [
    "# mqtt_ids_svm_train_rbf_bayes.py\n",
    "# Batch training of nonlinear RBF SVM with Bayesian optimization after each epoch.\n",
    "# - Reads CSV files in chunks (batches).\n",
    "# - Uses 3 epochs (configurable).\n",
    "# - After each epoch, runs Bayesian optimization (skopt.BayesSearchCV with GP surrogate\n",
    "#   and Expected Improvement acquisition) to tune C and gamma for RBF SVM.\n",
    "# - Validation/test pools are sampled & stored in-memory.\n",
    "#\n",
    "# NOTE: Install scikit-optimize for BayesSearchCV:\n",
    "#    pip install scikit-optimize\n",
    "# If scikit-optimize is not available, script falls back to GridSearchCV.\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import random\n",
    "\n",
    "# Try to import BayesSearchCV from scikit-optimize\n",
    "use_bayes = False\n",
    "try:\n",
    "    from skopt import BayesSearchCV\n",
    "    from skopt.space import Real, Categorical\n",
    "    use_bayes = True\n",
    "    print(\"[INFO] scikit-optimize detected: using BayesSearchCV (GP surrogate).\")\n",
    "except Exception:\n",
    "    print(\"[WARN] scikit-optimize not available. Falling back to GridSearchCV (grid search).\")\n",
    "\n",
    "# ------------------------------\n",
    "# Config\n",
    "# ------------------------------\n",
    "base_path = \"./\"\n",
    "\n",
    "folders = {\n",
    "    \"packet\": \"packet_features\",\n",
    "    \"uniflow\": \"uniflow_features\",\n",
    "    \"biflow\": \"biflow_features\"\n",
    "}\n",
    "\n",
    "files = {\n",
    "    \"normal\": \"normal.csv\",\n",
    "    \"sparta\": \"sparta.csv\",\n",
    "    \"scan_A\": \"scan_A.csv\",\n",
    "    \"mqtt_bruteforce\": \"mqtt_bruteforce.csv\",\n",
    "    \"scan_sU\": \"scan_sU.csv\"\n",
    "}\n",
    "\n",
    "def build_filenames(prefix):\n",
    "    return {\n",
    "        \"normal\": f\"{prefix}_normal.csv\",\n",
    "        \"sparta\": f\"{prefix}_sparta.csv\",\n",
    "        \"scan_A\": f\"{prefix}_scan_A.csv\",\n",
    "        \"mqtt_bruteforce\": f\"{prefix}_mqtt_bruteforce.csv\",\n",
    "        \"scan_sU\": f\"{prefix}_scan_sU.csv\"\n",
    "    }\n",
    "\n",
    "feature_files = {\n",
    "    \"packet\": files,\n",
    "    \"uniflow\": build_filenames(\"uniflow\"),\n",
    "    \"biflow\": build_filenames(\"biflow\")\n",
    "}\n",
    "\n",
    "CHUNKSIZE = 200000\n",
    "TRAIN_FRACTION = 0.80\n",
    "VAL_FRACTION = 0.10\n",
    "TEST_FRACTION = 0.10\n",
    "\n",
    "EPOCHS = 3                    # you requested 3 epochs\n",
    "SAMPLE_VAL_MAX = 20000\n",
    "SAMPLE_TEST_MAX = 20000\n",
    "\n",
    "# ðŸ”¹ Max number of samples allowed in training per epoch (editable by you)\n",
    "MAX_TRAIN_SAMPLES = 200000\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Search config for Bayes (per-epoch)\n",
    "BAYES_N_ITER = 25   # number of Bayesian iterations (evaluations) per epoch\n",
    "CV_FOLDS = 3\n",
    "SCORING = \"f1_macro\"\n",
    "\n",
    "# Fallback grid (used if skopt not installed)\n",
    "GRID_PARAM_GRID = {\n",
    "    \"C\": [0.1, 1, 10],\n",
    "    \"gamma\": [\"scale\", 0.01, 0.001],\n",
    "    \"kernel\": [\"rbf\"]\n",
    "}\n",
    "\n",
    "# ------------------------------\n",
    "# Helpers\n",
    "# ------------------------------\n",
    "def safe_remove(path):\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)\n",
    "            print(f\"[CLEANUP] Removed file: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[CLEANUP] Could not remove {path}: {e}\")\n",
    "\n",
    "def stream_chunks(feature_files_map, base_path, chunksize=CHUNKSIZE):\n",
    "    \"\"\"Yield (level, file_key, filepath, chunk_df).\"\"\"\n",
    "    for level, file_dict in feature_files_map.items():\n",
    "        folder_path = os.path.join(base_path, folders[level])\n",
    "        for key, fname in file_dict.items():\n",
    "            fpath = os.path.join(folder_path, fname)\n",
    "            if not os.path.isfile(fpath):\n",
    "                print(f\"[WARN] Missing: {fpath}\")\n",
    "                continue\n",
    "            try:\n",
    "                for chunk in pd.read_csv(fpath, chunksize=chunksize, low_memory=False):\n",
    "                    yield (level, key, fpath, chunk)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to read {fpath}: {e}\")\n",
    "\n",
    "def preprocess_chunk(df, file_key, expected_features=None):\n",
    "    \"\"\"\n",
    "    Preprocess chunk:\n",
    "      - set attack bool target column 'attack' (True if attack)\n",
    "      - select numeric features, drop 'attack' or 'label' if present\n",
    "      - reindex to expected_features (fill missing with 0) if provided\n",
    "      - return X, y, feature_names\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"attack\"] = (file_key != \"normal\")\n",
    "\n",
    "    y = df[\"attack\"].astype(int).values\n",
    "    numeric_df = df.select_dtypes(include=[np.number]).copy()\n",
    "    # drop columns that would leak target info\n",
    "    if \"attack\" in numeric_df.columns:\n",
    "        numeric_df = numeric_df.drop(columns=[\"attack\"])\n",
    "    if \"label\" in numeric_df.columns:\n",
    "        numeric_df = numeric_df.drop(columns=[\"label\"])\n",
    "\n",
    "    if expected_features is not None:\n",
    "        # ensure consistent ordering + fill missing\n",
    "        numeric_df = numeric_df.reindex(columns=expected_features, fill_value=0.0)\n",
    "        feature_names = expected_features\n",
    "    else:\n",
    "        feature_names = list(numeric_df.columns)\n",
    "\n",
    "    if len(feature_names) == 0:\n",
    "        # no numeric features\n",
    "        X = np.zeros((numeric_df.shape[0], 0), dtype=np.float32)\n",
    "    else:\n",
    "        X = numeric_df.fillna(0.0).values.astype(np.float32)\n",
    "\n",
    "    return X, y, feature_names\n",
    "\n",
    "# ------------------------------\n",
    "# Training loop with epochs + per-epoch Bayesian tuning\n",
    "# ------------------------------\n",
    "def train_svm_rbf_bayes(feature_files_map, base_path, epochs=EPOCHS):\n",
    "    scaler = StandardScaler(with_mean=False)  # sparse-friendly\n",
    "\n",
    "    feature_names_master = None\n",
    "    val_X = None; val_y = None\n",
    "    test_X = None; test_y = None\n",
    "\n",
    "    best_model = None\n",
    "    best_params_overall = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\n=== EPOCH {epoch}/{epochs} ===\")\n",
    "        # Collect training chunks for this epoch\n",
    "        train_chunks_X = []\n",
    "        train_chunks_y = []\n",
    "\n",
    "        # stream through all chunks (batches)\n",
    "        for level, file_key, filepath, chunk in stream_chunks(feature_files_map, base_path):\n",
    "            print(f\"[EPOCH {epoch}] CHUNK {level}/{file_key} shape={chunk.shape}\")\n",
    "            try:\n",
    "                if feature_names_master is None:\n",
    "                    Xc, yc, feat_names = preprocess_chunk(chunk, file_key, expected_features=None)\n",
    "                    feature_names_master = feat_names\n",
    "                    print(f\"[INFO] Established master feature count: {len(feature_names_master)}\")\n",
    "                else:\n",
    "                    Xc, yc, _ = preprocess_chunk(chunk, file_key, expected_features=feature_names_master)\n",
    "            except Exception as e:\n",
    "                print(f\"[SKIP] Preprocess error: {e}\")\n",
    "                continue\n",
    "\n",
    "            if Xc.shape[1] == 0 or Xc.shape[0] == 0:\n",
    "                # nothing to train on\n",
    "                print(f\"[SKIP] chunk has no numeric features or rows, skipping.\")\n",
    "                continue\n",
    "\n",
    "            # per-row random split into train/val/test\n",
    "            n = Xc.shape[0]\n",
    "            rnd = np.random.rand(n)\n",
    "            train_mask = rnd < TRAIN_FRACTION\n",
    "            val_mask = (rnd >= TRAIN_FRACTION) & (rnd < TRAIN_FRACTION + VAL_FRACTION)\n",
    "            test_mask = rnd >= TRAIN_FRACTION + VAL_FRACTION\n",
    "\n",
    "            X_train_chunk = Xc[train_mask]; y_train_chunk = yc[train_mask]\n",
    "            X_val_chunk = Xc[val_mask]; y_val_chunk = yc[val_mask]\n",
    "            X_test_chunk = Xc[test_mask]; y_test_chunk = yc[test_mask]\n",
    "\n",
    "            # collect training data (list of arrays -> vstack later)\n",
    "            if X_train_chunk.shape[0] > 0:\n",
    "                train_chunks_X.append(X_train_chunk)\n",
    "                train_chunks_y.append(y_train_chunk)\n",
    "\n",
    "            # maintain in-memory validation pool (sampled)\n",
    "            if X_val_chunk.shape[0] > 0:\n",
    "                if val_X is None:\n",
    "                    take = min(X_val_chunk.shape[0], SAMPLE_VAL_MAX)\n",
    "                    idxs = np.random.choice(X_val_chunk.shape[0], take, replace=False)\n",
    "                    val_X = X_val_chunk[idxs]; val_y = y_val_chunk[idxs]\n",
    "                else:\n",
    "                    val_X = np.vstack([val_X, X_val_chunk])\n",
    "                    val_y = np.concatenate([val_y, y_val_chunk])\n",
    "                    if val_X.shape[0] > SAMPLE_VAL_MAX:\n",
    "                        idxs = np.random.choice(val_X.shape[0], SAMPLE_VAL_MAX, replace=False)\n",
    "                        val_X = val_X[idxs]; val_y = val_y[idxs]\n",
    "\n",
    "            # maintain in-memory test pool (sampled)\n",
    "            if X_test_chunk.shape[0] > 0:\n",
    "                if test_X is None:\n",
    "                    take = min(X_test_chunk.shape[0], SAMPLE_TEST_MAX)\n",
    "                    idxs = np.random.choice(X_test_chunk.shape[0], take, replace=False)\n",
    "                    test_X = X_test_chunk[idxs]; test_y = y_test_chunk[idxs]\n",
    "                else:\n",
    "                    test_X = np.vstack([test_X, X_test_chunk])\n",
    "                    test_y = np.concatenate([test_y, y_test_chunk])\n",
    "                    if test_X.shape[0] > SAMPLE_TEST_MAX:\n",
    "                        idxs = np.random.choice(test_X.shape[0], SAMPLE_TEST_MAX, replace=False)\n",
    "                        test_X = test_X[idxs]; test_y = test_y[idxs]\n",
    "\n",
    "        # combine training chunks for this epoch\n",
    "        if len(train_chunks_X) == 0:\n",
    "            print(\"[WARN] No training data collected this epoch; skipping tuning/training.\")\n",
    "            continue\n",
    "\n",
    "        X_train = np.vstack(train_chunks_X)\n",
    "        y_train = np.concatenate(train_chunks_y)\n",
    "\n",
    "        # ðŸ”¹ Limit max training samples\n",
    "        if X_train.shape[0] > MAX_TRAIN_SAMPLES:\n",
    "            idxs = np.random.choice(X_train.shape[0], MAX_TRAIN_SAMPLES, replace=False)\n",
    "            X_train = X_train[idxs]\n",
    "            y_train = y_train[idxs]\n",
    "            print(f\"[INFO] Downsampled training set to {MAX_TRAIN_SAMPLES} samples.\")\n",
    "\n",
    "        # scale\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        if val_X is not None:\n",
    "            val_X_scaled = scaler.transform(val_X)\n",
    "        else:\n",
    "            val_X_scaled = None\n",
    "        if test_X is not None:\n",
    "            test_X_scaled = scaler.transform(test_X)\n",
    "        else:\n",
    "            test_X_scaled = None\n",
    "\n",
    "        # ------------------------------\n",
    "        # Bayesian optimization (or fallback grid) to tune C and gamma\n",
    "        # ------------------------------\n",
    "        if use_bayes:\n",
    "            # define search space (log-uniform for C and gamma)\n",
    "            search_spaces = {\n",
    "                \"C\": Real(1e-3, 1e3, prior=\"log-uniform\"),\n",
    "                \"gamma\": Real(1e-4, 1e1, prior=\"log-uniform\"),\n",
    "                \"kernel\": Categorical([\"rbf\"])\n",
    "            }\n",
    "\n",
    "            bayes = BayesSearchCV(\n",
    "                estimator=SVC(),\n",
    "                search_spaces=search_spaces,\n",
    "                n_iter=BAYES_N_ITER,\n",
    "                scoring=SCORING,\n",
    "                cv=CV_FOLDS,\n",
    "                n_jobs=-1,\n",
    "                verbose=0,\n",
    "                random_state=RANDOM_SEED,\n",
    "                optimizer_kwargs={\"acq_func\": \"EI\"}  # Expected Improvement acquisition\n",
    "            )\n",
    "\n",
    "            print(f\"[EPOCH {epoch}] Running BayesSearchCV (n_iter={BAYES_N_ITER}, cv={CV_FOLDS}) ...\")\n",
    "            bayes.fit(X_train, y_train)\n",
    "            best_est = bayes.best_estimator_\n",
    "            best_params = bayes.best_params_\n",
    "            best_score = bayes.best_score_\n",
    "            print(f\"[EPOCH {epoch}] Bayes best params: {best_params}  best CV {SCORING}={best_score:.4f}\")\n",
    "            best_model = best_est\n",
    "            best_params_overall = best_params\n",
    "        else:\n",
    "            # fallback to grid search\n",
    "            print(f\"[EPOCH {epoch}] scikit-optimize not installed; running GridSearchCV fallback.\")\n",
    "            grid = GridSearchCV(SVC(), GRID_PARAM_GRID, cv=CV_FOLDS, scoring=SCORING, n_jobs=-1, verbose=1)\n",
    "            grid.fit(X_train, y_train)\n",
    "            best_model = grid.best_estimator_\n",
    "            best_params_overall = grid.best_params_\n",
    "            print(f\"[EPOCH {epoch}] Grid best params: {best_params_overall}  best CV {SCORING}={grid.best_score_:.4f}\")\n",
    "\n",
    "        # Fit best_model on the entire epoch training set\n",
    "        try:\n",
    "            best_model.fit(X_train, y_train)\n",
    "        except Exception as e:\n",
    "            # in case best_model from BayesSearchCV is already fitted, ignore\n",
    "            print(f\"[WARN] best_model.fit failed (maybe already fitted): {e}\")\n",
    "\n",
    "        # Evaluate on validation pool\n",
    "        if val_X_scaled is not None:\n",
    "            preds_val = best_model.predict(val_X_scaled)\n",
    "            acc_val = accuracy_score(val_y, preds_val)\n",
    "            f1_val = f1_score(val_y, preds_val, average=\"macro\")\n",
    "            print(f\"[EPOCH {epoch}] Validation: Acc={acc_val:.4f}  F1_macro={f1_val:.4f}\")\n",
    "        else:\n",
    "            print(f\"[EPOCH {epoch}] No validation pool available to evaluate.\")\n",
    "\n",
    "        # Optionally evaluate on test pool (report only)\n",
    "        if test_X_scaled is not None:\n",
    "            preds_test = best_model.predict(test_X_scaled)\n",
    "            acc_test = accuracy_score(test_y, preds_test)\n",
    "            f1_test = f1_score(test_y, preds_test, average=\"macro\")\n",
    "            print(f\"[EPOCH {epoch}] Test (sampled pool): Acc={acc_test:.4f}  F1_macro={f1_test:.4f}\")\n",
    "\n",
    "        # Keep the best model from this epoch\n",
    "        best_model_epoch = best_model\n",
    "\n",
    "    # After epochs, final evaluation on test pool if available\n",
    "    final_model = best_model_epoch if 'best_model_epoch' in locals() else None\n",
    "    if final_model is not None and test_X is not None:\n",
    "        preds = final_model.predict(test_X_scaled)\n",
    "        acc = accuracy_score(test_y, preds)\n",
    "        f1 = f1_score(test_y, preds, average=\"macro\")\n",
    "        print(\"\\n=== FINAL EVALUATION ON TEST POOL ===\")\n",
    "        print(f\"Test Acc={acc:.4f}  F1_macro={f1:.4f}\")\n",
    "        print(classification_report(test_y, preds, target_names=[\"Secure(False)\", \"Attack(True)\"]))\n",
    "    else:\n",
    "        print(\"No final model or test pool to evaluate.\")\n",
    "\n",
    "    return final_model, feature_names_master, best_params_overall\n",
    "\n",
    "# ------------------------------\n",
    "# Main\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    print(\"[START] Training RBF SVM with per-epoch Bayesian optimization (epochs={})...\".format(EPOCHS))\n",
    "    try:\n",
    "        svm_model, features, best_params = train_svm_rbf_bayes(feature_files, base_path, epochs=EPOCHS)\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Training aborted: {e}\")\n",
    "        svm_model, features, best_params = None, None, None\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"[DONE] Total time: {elapsed:.1f}s\")\n",
    "    if best_params is not None:\n",
    "        print(f\"[RESULT] Best params (last epoch): {best_params}\")\n",
    "\n",
    "    # Cleanup temp files patterns in system temp dir\n",
    "    print(\"\\n[START CLEANUP]\")\n",
    "    tmpdir = tempfile.gettempdir()\n",
    "    for pattern in [\"*.tmp\", \"*.temp\", \"tmp*\"]:\n",
    "        for f in glob.glob(os.path.join(tmpdir, pattern)):\n",
    "            safe_remove(f)\n",
    "    print(\"[CLEANUP] Completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seawsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
