{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3726ec18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EPOCH 1/3 ===\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No numeric features.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 195\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# Run\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m==\u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m     \u001b[43mtrain_boosted_tree_batchwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 139\u001b[39m, in \u001b[36mtrain_boosted_tree_batchwise\u001b[39m\u001b[34m(feature_files_map, base_path, epochs)\u001b[39m\n\u001b[32m    137\u001b[39m     Xb,yb,feature_names_master = preprocess_chunk(chunk,file_key)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     Xb,yb,_ = \u001b[43mpreprocess_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfile_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43mexpected_feature_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_names_master\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m n = Xb.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    141\u001b[39m rnd = np.random.rand(n)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 99\u001b[39m, in \u001b[36mpreprocess_chunk\u001b[39m\u001b[34m(df, file_key, expected_feature_names)\u001b[39m\n\u001b[32m     97\u001b[39m     feature_names = \u001b[38;5;28mlist\u001b[39m(numeric_df.columns)\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(feature_names) == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo numeric features.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    100\u001b[39m numeric_df = numeric_df.fillna(\u001b[32m0.0\u001b[39m)\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m numeric_df.values.astype(np.float32), y_encoded.astype(np.int32), feature_names\n",
      "\u001b[31mValueError\u001b[39m: No numeric features."
     ]
    }
   ],
   "source": [
    "# mqtt_ids_boost_train_bayes.py\n",
    "# Batch + epoch training of boosted decision tree (XGBoost) for multi-class detection\n",
    "# with Bayesian optimization of hyperparameters (per-epoch using validation set).\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import xgboost as xgb\n",
    "import random\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "# ------------------------------\n",
    "# Config\n",
    "# ------------------------------\n",
    "base_path = \"./\"\n",
    "folders = {\"packet\": \"packet_features\", \"uniflow\": \"uniflow_features\", \"biflow\": \"biflow_features\"}\n",
    "files = {\"normal\": \"normal.csv\", \"sparta\": \"sparta.csv\", \"scan_A\": \"scan_A.csv\", \"mqtt_bruteforce\": \"mqtt_bruteforce.csv\", \"scan_sU\": \"scan_sU.csv\"}\n",
    "\n",
    "def build_filenames(prefix):\n",
    "    return {k: f\"{prefix}_{v}\" for k,v in files.items()}\n",
    "\n",
    "feature_files = {\"packet\": files, \"uniflow\": build_filenames(\"uniflow\"), \"biflow\": build_filenames(\"biflow\")}\n",
    "\n",
    "# Training params\n",
    "CHUNKSIZE = 200000\n",
    "TRAIN_FRACTION, VAL_FRACTION, TEST_FRACTION = 0.80, 0.10, 0.10\n",
    "EPOCHS = 3\n",
    "ROUNDS_PER_BATCH = 1\n",
    "SAMPLE_VAL_MAX, SAMPLE_TEST_MAX = 20000, 20000\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# ------------------------------\n",
    "# Hyperparameter search space for Bayesian optimization\n",
    "# ------------------------------\n",
    "space  = [\n",
    "    Real(0.01, 0.3, name='eta'),\n",
    "    Integer(3, 10, name='max_depth'),\n",
    "    Real(0.5, 1.0, name='subsample'),\n",
    "    Real(0.5, 1.0, name='colsample_bytree')\n",
    "]\n",
    "\n",
    "# Initial defaults\n",
    "current_best_params = {\"eta\": 0.1, \"max_depth\": 6, \"subsample\": 1.0, \"colsample_bytree\": 1.0}\n",
    "\n",
    "XGB_FIXED_PARAMS = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"verbosity\": 0\n",
    "}\n",
    "\n",
    "# ------------------------------\n",
    "# Label encoding\n",
    "# ------------------------------\n",
    "attack_type_names = [\"normal\", \"sparta\", \"scan_A\", \"mqtt_bruteforce\", \"scan_sU\"]\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(attack_type_names)\n",
    "\n",
    "# ------------------------------\n",
    "# Stream CSVs\n",
    "# ------------------------------\n",
    "def stream_chunks_for_all_files(feature_files_map, base_path, chunksize=CHUNKSIZE):\n",
    "    for level, file_dict in feature_files_map.items():\n",
    "        folder_path = os.path.join(base_path, folders[level])\n",
    "        for key, fname in file_dict.items():\n",
    "            fpath = os.path.join(folder_path, fname)\n",
    "            if not os.path.isfile(fpath):\n",
    "                continue\n",
    "            for chunk in pd.read_csv(fpath, chunksize=chunksize, low_memory=False):\n",
    "                yield (level, key, fpath, chunk)\n",
    "\n",
    "# ------------------------------\n",
    "# Preprocess\n",
    "# ------------------------------\n",
    "def preprocess_chunk(df, file_key, expected_feature_names=None):\n",
    "    df = df.copy()\n",
    "    df[\"attack_type\"] = file_key\n",
    "    y_encoded = label_encoder.transform(df[\"attack_type\"].astype(str).values)\n",
    "    numeric_df = df.select_dtypes(include=[np.number]).copy()\n",
    "    for col in [\"label\",\"attack_type\"]:\n",
    "        if col in numeric_df.columns:\n",
    "            numeric_df = numeric_df.drop(columns=[col])\n",
    "    if expected_feature_names is not None:\n",
    "        numeric_df = numeric_df[[c for c in expected_feature_names if c in numeric_df.columns]]\n",
    "        feature_names = [c for c in expected_feature_names if c in numeric_df.columns]\n",
    "    else:\n",
    "        feature_names = list(numeric_df.columns)\n",
    "    if len(feature_names) == 0:\n",
    "        raise ValueError(\"No numeric features.\")\n",
    "    numeric_df = numeric_df.fillna(0.0)\n",
    "    return numeric_df.values.astype(np.float32), y_encoded.astype(np.int32), feature_names\n",
    "\n",
    "# ------------------------------\n",
    "# Bayesian objective function (per-epoch)\n",
    "# ------------------------------\n",
    "val_X_global, val_y_global, feature_names_master = None, None, None\n",
    "booster_snapshot = None\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    global booster_snapshot, val_X_global, val_y_global, feature_names_master\n",
    "    dval = xgb.DMatrix(val_X_global, label=val_y_global, feature_names=feature_names_master)\n",
    "    booster = xgb.train(\n",
    "        params={**XGB_FIXED_PARAMS, **params, \"num_class\": len(label_encoder.classes_)},\n",
    "        dtrain=dval, num_boost_round=3, xgb_model=booster_snapshot, verbose_eval=False\n",
    "    )\n",
    "    preds = np.argmax(booster.predict(dval), axis=1)\n",
    "    f1 = f1_score(val_y_global, preds, average=\"macro\")\n",
    "    return -f1   # skopt minimizes\n",
    "\n",
    "# ------------------------------\n",
    "# Main training loop\n",
    "# ------------------------------\n",
    "def train_boosted_tree_batchwise(feature_files_map, base_path, epochs=EPOCHS):\n",
    "    global val_X_global, val_y_global, feature_names_master, booster_snapshot, current_best_params\n",
    "\n",
    "    booster = None\n",
    "    val_X, val_y, test_X, test_y = None, None, None, None\n",
    "    total_batches = 0\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(f\"\\n=== EPOCH {epoch}/{epochs} ===\")\n",
    "        gen = stream_chunks_for_all_files(feature_files_map, base_path)\n",
    "        for level, file_key, filepath, chunk in gen:\n",
    "            total_batches += 1\n",
    "            if feature_names_master is None:\n",
    "                Xb,yb,feature_names_master = preprocess_chunk(chunk,file_key)\n",
    "            else:\n",
    "                Xb,yb,_ = preprocess_chunk(chunk,file_key,expected_feature_names=feature_names_master)\n",
    "            n = Xb.shape[0]\n",
    "            rnd = np.random.rand(n)\n",
    "            train_mask = rnd<TRAIN_FRACTION\n",
    "            val_mask = (rnd>=TRAIN_FRACTION)&(rnd<TRAIN_FRACTION+VAL_FRACTION)\n",
    "            test_mask = rnd>=TRAIN_FRACTION+VAL_FRACTION\n",
    "\n",
    "            X_train,y_train = Xb[train_mask], yb[train_mask]\n",
    "            X_val,y_val = Xb[val_mask], yb[val_mask]\n",
    "            X_test,y_test = Xb[test_mask], yb[test_mask]\n",
    "\n",
    "            if X_val.shape[0]>0:\n",
    "                val_X = X_val if val_X is None else np.vstack([val_X,X_val])\n",
    "                val_y = y_val if val_y is None else np.concatenate([val_y,y_val])\n",
    "                if val_X.shape[0]>SAMPLE_VAL_MAX:\n",
    "                    idx = np.random.choice(val_X.shape[0],SAMPLE_VAL_MAX,replace=False)\n",
    "                    val_X,val_y = val_X[idx],val_y[idx]\n",
    "\n",
    "            if X_test.shape[0]>0:\n",
    "                test_X = X_test if test_X is None else np.vstack([test_X,X_test])\n",
    "                test_y = y_test if test_y is None else np.concatenate([test_y,y_test])\n",
    "                if test_X.shape[0]>SAMPLE_TEST_MAX:\n",
    "                    idx = np.random.choice(test_X.shape[0],SAMPLE_TEST_MAX,replace=False)\n",
    "                    test_X,test_y = test_X[idx],test_y[idx]\n",
    "\n",
    "            if X_train.shape[0]==0: continue\n",
    "\n",
    "            dtrain = xgb.DMatrix(X_train,label=y_train,feature_names=feature_names_master)\n",
    "            booster = xgb.train(\n",
    "                params={**XGB_FIXED_PARAMS, **current_best_params, \"num_class\": len(label_encoder.classes_)},\n",
    "                dtrain=dtrain,num_boost_round=ROUNDS_PER_BATCH,xgb_model=booster,verbose_eval=False)\n",
    "            booster_snapshot = booster\n",
    "\n",
    "        # --- Hyperparameter tuning after epoch ---\n",
    "        if val_X is not None:\n",
    "            val_X_global, val_y_global = val_X, val_y\n",
    "            print(\"[Bayesian Opt] Running hyperparameter tuning on validation set...\")\n",
    "            res = gp_minimize(objective, space, n_calls=10, random_state=RANDOM_SEED, acq_func=\"EI\")\n",
    "            best_params = {dim.name: val for dim,val in zip(space,res.x)}\n",
    "            current_best_params.update(best_params)\n",
    "            print(f\"[EPOCH {epoch}] Best hyperparams: {current_best_params}\")\n",
    "\n",
    "    if test_X is not None and booster is not None:\n",
    "        dtest = xgb.DMatrix(test_X,label=test_y,feature_names=feature_names_master)\n",
    "        preds = np.argmax(booster.predict(dtest),axis=1)\n",
    "        acc = accuracy_score(test_y,preds)\n",
    "        f1 = f1_score(test_y,preds,average=\"macro\")\n",
    "        print(\"\\n=== FINAL TEST ===\")\n",
    "        print(f\"Accuracy={acc:.4f} F1={f1:.4f}\")\n",
    "        print(classification_report(test_y,preds,target_names=label_encoder.classes_))\n",
    "    return booster, feature_names_master\n",
    "\n",
    "# ------------------------------\n",
    "# Run\n",
    "# ------------------------------\n",
    "if __name__==\"__main__\":\n",
    "    train_boosted_tree_batchwise(feature_files, base_path, epochs=EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seawsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
