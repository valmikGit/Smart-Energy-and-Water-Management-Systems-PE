{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3726ec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mqtt_ids_boost_train_bayes.py\n",
    "# Batch + epoch training of boosted decision tree (XGBoost) for multi-class detection\n",
    "# with Bayesian optimization of hyperparameters (per-epoch using validation set).\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import xgboost as xgb\n",
    "import random\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "# ------------------------------\n",
    "# Config\n",
    "# ------------------------------\n",
    "base_path = \"./\"\n",
    "folders = {\n",
    "    \"packet\": \"packet_features\",\n",
    "    \"uniflow\": \"uniflow_features\",\n",
    "    \"biflow\": \"biflow_features\",\n",
    "}\n",
    "files = {\n",
    "    \"normal\": \"normal.csv\",\n",
    "    \"sparta\": \"sparta.csv\",\n",
    "    \"scan_A\": \"scan_A.csv\",\n",
    "    \"mqtt_bruteforce\": \"mqtt_bruteforce.csv\",\n",
    "    \"scan_sU\": \"scan_sU.csv\",\n",
    "}\n",
    "\n",
    "\n",
    "def build_filenames(prefix):\n",
    "    return {k: f\"{prefix}_{v}\" for k, v in files.items()}\n",
    "\n",
    "\n",
    "feature_files = {\n",
    "    \"packet\": files,\n",
    "    \"uniflow\": build_filenames(\"uniflow\"),\n",
    "    \"biflow\": build_filenames(\"biflow\"),\n",
    "}\n",
    "\n",
    "# Training params\n",
    "CHUNKSIZE = 200000\n",
    "TRAIN_FRACTION, VAL_FRACTION, TEST_FRACTION = 0.80, 0.10, 0.10\n",
    "EPOCHS = 3\n",
    "ROUNDS_PER_BATCH = 1\n",
    "SAMPLE_VAL_MAX, SAMPLE_TEST_MAX = 20000, 20000\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# ------------------------------\n",
    "# Hyperparameter search space for Bayesian optimization\n",
    "# ------------------------------\n",
    "space = [\n",
    "    Real(0.01, 0.3, name=\"eta\"),\n",
    "    Integer(3, 10, name=\"max_depth\"),\n",
    "    Real(0.5, 1.0, name=\"subsample\"),\n",
    "    Real(0.5, 1.0, name=\"colsample_bytree\"),\n",
    "]\n",
    "\n",
    "# Initial defaults\n",
    "current_best_params = {\n",
    "    \"eta\": 0.1,\n",
    "    \"max_depth\": 6,\n",
    "    \"subsample\": 1.0,\n",
    "    \"colsample_bytree\": 1.0,\n",
    "}\n",
    "\n",
    "XGB_FIXED_PARAMS = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"verbosity\": 0,\n",
    "}\n",
    "\n",
    "# ------------------------------\n",
    "# Label encoding\n",
    "# ------------------------------\n",
    "attack_type_names = [\"normal\", \"sparta\", \"scan_A\", \"mqtt_bruteforce\", \"scan_sU\"]\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(attack_type_names)\n",
    "\n",
    "# ------------------------------\n",
    "# Stream CSVs\n",
    "# ------------------------------\n",
    "def stream_chunks_for_all_files(feature_files_map, base_path, chunksize=CHUNKSIZE):\n",
    "    for level, file_dict in feature_files_map.items():\n",
    "        folder_path = os.path.join(base_path, folders[level])\n",
    "        for key, fname in file_dict.items():\n",
    "            fpath = os.path.join(folder_path, fname)\n",
    "            if not os.path.isfile(fpath):\n",
    "                continue\n",
    "            for chunk in pd.read_csv(fpath, chunksize=chunksize, low_memory=False):\n",
    "                yield (level, key, fpath, chunk)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Preprocess\n",
    "# ------------------------------\n",
    "def preprocess_chunk(df, file_key, expected_feature_names=None):\n",
    "    df = df.copy()\n",
    "    df[\"attack_type\"] = file_key\n",
    "    y_encoded = label_encoder.transform(df[\"attack_type\"].astype(str).values)\n",
    "\n",
    "    numeric_df = df.select_dtypes(include=[np.number]).copy()\n",
    "    for col in [\"label\", \"attack_type\"]:\n",
    "        if col in numeric_df.columns:\n",
    "            numeric_df = numeric_df.drop(columns=[col])\n",
    "\n",
    "    if expected_feature_names is not None:\n",
    "        # Reindex to expected features, filling missing with zeros\n",
    "        numeric_df = numeric_df.reindex(columns=expected_feature_names, fill_value=0.0)\n",
    "        feature_names = list(expected_feature_names)\n",
    "    else:\n",
    "        feature_names = list(numeric_df.columns)\n",
    "\n",
    "    numeric_df = numeric_df.fillna(0.0)\n",
    "    return numeric_df.values.astype(np.float32), y_encoded.astype(np.int32), feature_names\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Bayesian objective function (per-epoch)\n",
    "# ------------------------------\n",
    "val_X_global, val_y_global, feature_names_master = None, None, None\n",
    "booster_snapshot = None\n",
    "\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    global booster_snapshot, val_X_global, val_y_global, feature_names_master\n",
    "    dval = xgb.DMatrix(\n",
    "        val_X_global, label=val_y_global, feature_names=feature_names_master\n",
    "    )\n",
    "    booster = xgb.train(\n",
    "        params={**XGB_FIXED_PARAMS, **params, \"num_class\": len(label_encoder.classes_)},\n",
    "        dtrain=dval,\n",
    "        num_boost_round=3,\n",
    "        xgb_model=booster_snapshot,\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "    preds = np.argmax(booster.predict(dval), axis=1)\n",
    "    f1 = f1_score(val_y_global, preds, average=\"macro\")\n",
    "    return -f1  # skopt minimizes\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Main training loop\n",
    "# ------------------------------\n",
    "def train_boosted_tree_batchwise(feature_files_map, base_path, epochs=EPOCHS):\n",
    "    global val_X_global, val_y_global, feature_names_master, booster_snapshot, current_best_params\n",
    "\n",
    "    booster = None\n",
    "    val_X, val_y, test_X, test_y = None, None, None, None\n",
    "    total_batches = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\n=== EPOCH {epoch}/{epochs} ===\")\n",
    "        gen = stream_chunks_for_all_files(feature_files_map, base_path)\n",
    "        for level, file_key, filepath, chunk in gen:\n",
    "            total_batches += 1\n",
    "            if feature_names_master is None:\n",
    "                Xb, yb, feature_names_master = preprocess_chunk(chunk, file_key)\n",
    "            else:\n",
    "                Xb, yb, _ = preprocess_chunk(\n",
    "                    chunk, file_key, expected_feature_names=feature_names_master\n",
    "                )\n",
    "            n = Xb.shape[0]\n",
    "            rnd = np.random.rand(n)\n",
    "            train_mask = rnd < TRAIN_FRACTION\n",
    "            val_mask = (rnd >= TRAIN_FRACTION) & (rnd < TRAIN_FRACTION + VAL_FRACTION)\n",
    "            test_mask = rnd >= TRAIN_FRACTION + VAL_FRACTION\n",
    "\n",
    "            X_train, y_train = Xb[train_mask], yb[train_mask]\n",
    "            X_val, y_val = Xb[val_mask], yb[val_mask]\n",
    "            X_test, y_test = Xb[test_mask], yb[test_mask]\n",
    "\n",
    "            if X_val.shape[0] > 0:\n",
    "                val_X = X_val if val_X is None else np.vstack([val_X, X_val])\n",
    "                val_y = y_val if val_y is None else np.concatenate([val_y, y_val])\n",
    "                if val_X.shape[0] > SAMPLE_VAL_MAX:\n",
    "                    idx = np.random.choice(val_X.shape[0], SAMPLE_VAL_MAX, replace=False)\n",
    "                    val_X, val_y = val_X[idx], val_y[idx]\n",
    "\n",
    "            if X_test.shape[0] > 0:\n",
    "                test_X = X_test if test_X is None else np.vstack([test_X, X_test])\n",
    "                test_y = y_test if test_y is None else np.concatenate([test_y, y_test])\n",
    "                if test_X.shape[0] > SAMPLE_TEST_MAX:\n",
    "                    idx = np.random.choice(test_X.shape[0], SAMPLE_TEST_MAX, replace=False)\n",
    "                    test_X, test_y = test_X[idx], test_y[idx]\n",
    "\n",
    "            if X_train.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            dtrain = xgb.DMatrix(\n",
    "                X_train, label=y_train, feature_names=feature_names_master\n",
    "            )\n",
    "            booster = xgb.train(\n",
    "                params={\n",
    "                    **XGB_FIXED_PARAMS,\n",
    "                    **current_best_params,\n",
    "                    \"num_class\": len(label_encoder.classes_),\n",
    "                },\n",
    "                dtrain=dtrain,\n",
    "                num_boost_round=ROUNDS_PER_BATCH,\n",
    "                xgb_model=booster,\n",
    "                verbose_eval=False,\n",
    "            )\n",
    "            booster_snapshot = booster\n",
    "\n",
    "        # --- Hyperparameter tuning after epoch ---\n",
    "        if val_X is not None:\n",
    "            val_X_global, val_y_global = val_X, val_y\n",
    "            print(\"[Bayesian Opt] Running hyperparameter tuning on validation set...\")\n",
    "            res = gp_minimize(\n",
    "                objective,\n",
    "                space,\n",
    "                n_calls=10,\n",
    "                random_state=RANDOM_SEED,\n",
    "                acq_func=\"EI\",\n",
    "            )\n",
    "            best_params = {dim.name: val for dim, val in zip(space, res.x)}\n",
    "            current_best_params.update(best_params)\n",
    "            print(f\"[EPOCH {epoch}] Best hyperparams: {current_best_params}\")\n",
    "\n",
    "    if test_X is not None and booster is not None:\n",
    "        dtest = xgb.DMatrix(test_X, label=test_y, feature_names=feature_names_master)\n",
    "        preds = np.argmax(booster.predict(dtest), axis=1)\n",
    "        acc = accuracy_score(test_y, preds)\n",
    "        f1 = f1_score(test_y, preds, average=\"macro\")\n",
    "        print(\"\\n=== FINAL TEST ===\")\n",
    "        print(f\"Accuracy={acc:.4f} F1={f1:.4f}\")\n",
    "        print(classification_report(test_y, preds, target_names=label_encoder.classes_))\n",
    "    return booster, feature_names_master\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Run\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    train_boosted_tree_batchwise(feature_files, base_path, epochs=EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seawsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
