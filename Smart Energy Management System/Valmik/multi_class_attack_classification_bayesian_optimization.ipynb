{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3726ec18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COLLECT] Collected 50000 samples with 27 features.\n",
      "[SPLIT] total=50000, train=40000, val=5000, test=5000\n",
      "\n",
      "=== EPOCH 1/3 ===\n",
      "  [epoch 1] processed 10/10 batches\n",
      "[Bayesian Opt] Running hyperparameter tuning on validation set...\n",
      "[Bayes] Done in 2.7s. Best objective value=-1.000000\n",
      "[EPOCH 1] Updated best params: {'eta': 0.24099746618946757, 'max_depth': np.int64(4), 'subsample': 0.8898455001363847, 'colsample_bytree': 0.7984250789732436}\n",
      "\n",
      "=== EPOCH 2/3 ===\n",
      "  [epoch 2] processed 10/10 batches\n",
      "[Bayesian Opt] Running hyperparameter tuning on validation set...\n",
      "[Bayes] Done in 2.6s. Best objective value=-1.000000\n",
      "[EPOCH 2] Updated best params: {'eta': 0.24099746618946757, 'max_depth': np.int64(4), 'subsample': 0.8898455001363847, 'colsample_bytree': 0.7984250789732436}\n",
      "\n",
      "=== EPOCH 3/3 ===\n",
      "  [epoch 3] processed 10/10 batches\n",
      "[Bayesian Opt] Running hyperparameter tuning on validation set...\n",
      "[Bayes] Done in 2.8s. Best objective value=-1.000000\n",
      "[EPOCH 3] Updated best params: {'eta': 0.24099746618946757, 'max_depth': np.int64(4), 'subsample': 0.8898455001363847, 'colsample_bytree': 0.7984250789732436}\n",
      "\n",
      "=== FINAL TEST ===\n",
      "Accuracy=1.0000  F1=1.0000\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "mqtt_bruteforce       0.00      0.00      0.00         0\n",
      "         normal       1.00      1.00      1.00      5000\n",
      "         scan_A       0.00      0.00      0.00         0\n",
      "        scan_sU       0.00      0.00      0.00         0\n",
      "         sparta       0.00      0.00      0.00         0\n",
      "\n",
      "       accuracy                           1.00      5000\n",
      "      macro avg       0.20      0.20      0.20      5000\n",
      "   weighted avg       1.00      1.00      1.00      5000\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# mqtt_ids_boost_train_bayes_fixed.py\n",
    "# Batch + epoch training of boosted decision tree (XGBoost) for multi-class detection\n",
    "# with Bayesian optimization of hyperparameters (per-epoch using validation set).\n",
    "# Fixes:\n",
    "#  - global train/val/test split (no per-chunk splitting)\n",
    "#  - Bayesian objective trains on training set only and evaluates on validation set\n",
    "#  - optional MAX_SAMPLES cap to limit memory usage\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import xgboost as xgb\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "# ------------------------------\n",
    "# Config (edit as needed)\n",
    "# ------------------------------\n",
    "base_path = \"./\"\n",
    "folders = {\n",
    "    \"packet\": \"packet_features\",\n",
    "    \"uniflow\": \"uniflow_features\",\n",
    "    \"biflow\": \"biflow_features\",\n",
    "}\n",
    "files = {\n",
    "    \"normal\": \"normal.csv\",\n",
    "    \"sparta\": \"sparta.csv\",\n",
    "    \"scan_A\": \"scan_A.csv\",\n",
    "    \"mqtt_bruteforce\": \"mqtt_bruteforce.csv\",\n",
    "    \"scan_sU\": \"scan_sU.csv\",\n",
    "}\n",
    "\n",
    "\n",
    "def build_filenames(prefix):\n",
    "    return {k: f\"{prefix}_{v}\" for k, v in files.items()}\n",
    "\n",
    "\n",
    "feature_files = {\n",
    "    \"packet\": files,\n",
    "    \"uniflow\": build_filenames(\"uniflow\"),\n",
    "    \"biflow\": build_filenames(\"biflow\"),\n",
    "}\n",
    "\n",
    "# training params\n",
    "CHUNKSIZE = 2000\n",
    "TRAIN_FRACTION, VAL_FRACTION, TEST_FRACTION = 0.80, 0.10, 0.10\n",
    "EPOCHS = 3\n",
    "ROUNDS_PER_BATCH = 2     # how many boosting rounds per batch update\n",
    "SAMPLE_VAL_MAX, SAMPLE_TEST_MAX = 20000, 20000\n",
    "\n",
    "# Limit total samples processed (set to None for unlimited)\n",
    "MAX_SAMPLES = 50000   # set to None to process entire dataset\n",
    "\n",
    "BATCH_SIZE = 4096     # number of rows used per incremental update to XGBoost (not ML minibatch for NN)\n",
    "GP_N_CALLS = 10       # gp_minimize calls per epoch\n",
    "GP_INIT_POINTS = 3\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# ------------------------------\n",
    "# Hyperparameter search space for Bayesian optimization\n",
    "# ------------------------------\n",
    "space = [\n",
    "    Real(0.01, 0.3, name=\"eta\"),\n",
    "    Integer(3, 10, name=\"max_depth\"),\n",
    "    Real(0.5, 1.0, name=\"subsample\"),\n",
    "    Real(0.5, 1.0, name=\"colsample_bytree\"),\n",
    "]\n",
    "\n",
    "# initial defaults\n",
    "current_best_params = {\n",
    "    \"eta\": 0.1,\n",
    "    \"max_depth\": 6,\n",
    "    \"subsample\": 1.0,\n",
    "    \"colsample_bytree\": 1.0,\n",
    "}\n",
    "\n",
    "XGB_FIXED_PARAMS = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"verbosity\": 0,\n",
    "}\n",
    "\n",
    "# ------------------------------\n",
    "# Label encoding\n",
    "# ------------------------------\n",
    "attack_type_names = [\"normal\", \"sparta\", \"scan_A\", \"mqtt_bruteforce\", \"scan_sU\"]\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(attack_type_names)\n",
    "\n",
    "# ------------------------------\n",
    "# Stream CSVs utility\n",
    "# ------------------------------\n",
    "def stream_chunks_for_all_files(feature_files_map, base_path, chunksize=CHUNKSIZE):\n",
    "    \"\"\"\n",
    "    Generator yielding (level, key, filepath, chunk_df)\n",
    "    \"\"\"\n",
    "    for level, file_dict in feature_files_map.items():\n",
    "        folder_path = os.path.join(base_path, folders[level])\n",
    "        for key, fname in file_dict.items():\n",
    "            fpath = os.path.join(folder_path, fname)\n",
    "            if not os.path.isfile(fpath):\n",
    "                continue\n",
    "            for chunk in pd.read_csv(fpath, chunksize=chunksize, low_memory=False):\n",
    "                yield (level, key, fpath, chunk)\n",
    "\n",
    "# ------------------------------\n",
    "# Preprocess chunk\n",
    "# ------------------------------\n",
    "def preprocess_chunk(df, file_key, expected_feature_names=None):\n",
    "    \"\"\"\n",
    "    Returns (X_numpy, y_numpy, feature_names_list)\n",
    "    Ensures numeric-only features, consistent column order via expected_feature_names (if provided).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"attack_type\"] = file_key\n",
    "    # target (text -> encoded int)\n",
    "    y_encoded = label_encoder.transform(df[\"attack_type\"].astype(str).values)\n",
    "\n",
    "    # numeric features only, drop potential numeric label columns that leak\n",
    "    numeric_df = df.select_dtypes(include=[np.number]).copy()\n",
    "    for col in [\"label\", \"attack_type\"]:\n",
    "        if col in numeric_df.columns:\n",
    "            numeric_df = numeric_df.drop(columns=[col])\n",
    "\n",
    "    if expected_feature_names is not None:\n",
    "        # reindex to the master feature set, filling missing with zeros\n",
    "        numeric_df = numeric_df.reindex(columns=expected_feature_names, fill_value=0.0)\n",
    "        feature_names = list(expected_feature_names)\n",
    "    else:\n",
    "        feature_names = list(numeric_df.columns)\n",
    "\n",
    "    # fill NaN\n",
    "    numeric_df = numeric_df.fillna(0.0)\n",
    "    X = numeric_df.values.astype(np.float32)\n",
    "    y = y_encoded.astype(np.int32)\n",
    "    return X, y, feature_names\n",
    "\n",
    "# ------------------------------\n",
    "# Collect up to MAX_SAMPLES globally (streaming)\n",
    "# ------------------------------\n",
    "def collect_samples(feature_files_map, base_path, max_samples=None):\n",
    "    \"\"\"\n",
    "    Stream through CSVs and collect up to max_samples rows into X_all and y_all.\n",
    "    Returns X_all (numpy), y_all (numpy), feature_names_master (list).\n",
    "    \"\"\"\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    feature_names_master = None\n",
    "    samples_collected = 0\n",
    "\n",
    "    for level, key, path, chunk in stream_chunks_for_all_files(feature_files_map, base_path):\n",
    "        # preprocess chunk to numeric features\n",
    "        if feature_names_master is None:\n",
    "            Xb, yb, feature_names_master = preprocess_chunk(chunk, key, expected_feature_names=None)\n",
    "        else:\n",
    "            Xb, yb, _ = preprocess_chunk(chunk, key, expected_feature_names=feature_names_master)\n",
    "\n",
    "        if Xb.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        # figure out how many to take from this chunk\n",
    "        if max_samples is not None:\n",
    "            remaining = max_samples - samples_collected\n",
    "            if remaining <= 0:\n",
    "                break\n",
    "            if Xb.shape[0] > remaining:\n",
    "                idx = np.random.choice(Xb.shape[0], remaining, replace=False)\n",
    "                Xb = Xb[idx]\n",
    "                yb = yb[idx]\n",
    "\n",
    "        X_list.append(Xb)\n",
    "        y_list.append(yb)\n",
    "        samples_collected += Xb.shape[0]\n",
    "\n",
    "        if max_samples is not None and samples_collected >= max_samples:\n",
    "            break\n",
    "\n",
    "    if samples_collected == 0:\n",
    "        raise RuntimeError(\"No samples found while collecting data. Check paths and CSVs.\")\n",
    "\n",
    "    X_all = np.vstack(X_list)\n",
    "    y_all = np.concatenate(y_list)\n",
    "    print(f\"[COLLECT] Collected {X_all.shape[0]} samples with {len(feature_names_master)} features.\")\n",
    "    return X_all, y_all, feature_names_master\n",
    "\n",
    "# ------------------------------\n",
    "# Bayesian objective (train on train set and score on val set)\n",
    "# ------------------------------\n",
    "X_train_global = None\n",
    "y_train_global = None\n",
    "X_val_global = None\n",
    "y_val_global = None\n",
    "feature_names_master_global = None\n",
    "num_class_global = None\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(eta, max_depth, subsample, colsample_bytree):\n",
    "    \"\"\"\n",
    "    Objective function for gp_minimize. Trains on training set, evaluates on validation set.\n",
    "    \"\"\"\n",
    "    global X_train_global, y_train_global, X_val_global, y_val_global, feature_names_master_global, num_class_global\n",
    "\n",
    "    if X_train_global is None or X_val_global is None:\n",
    "        return 1e6  # fail safe\n",
    "\n",
    "    params = {\n",
    "        **XGB_FIXED_PARAMS,\n",
    "        \"eta\": float(eta),\n",
    "        \"max_depth\": int(max_depth),\n",
    "        \"subsample\": float(subsample),\n",
    "        \"colsample_bytree\": float(colsample_bytree),\n",
    "        \"num_class\": int(num_class_global)\n",
    "    }\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train_global, label=y_train_global, feature_names=feature_names_master_global)\n",
    "    dval = xgb.DMatrix(X_val_global, label=y_val_global, feature_names=feature_names_master_global)\n",
    "\n",
    "    try:\n",
    "        booster = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=50,\n",
    "            evals=[(dval, \"val\")],\n",
    "            early_stopping_rounds=5,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[BO-OBJ] xgb.train failed: {e}\")\n",
    "        return 1e6\n",
    "\n",
    "    preds_prob = booster.predict(dval)\n",
    "    preds = np.argmax(preds_prob, axis=1)\n",
    "    f1 = f1_score(y_val_global, preds, average=\"macro\")\n",
    "    return -float(f1)\n",
    "\n",
    "# ------------------------------\n",
    "# Main training (use global split and batch updates)\n",
    "# ------------------------------\n",
    "def train_boosted_tree_batchwise(feature_files_map, base_path, epochs=EPOCHS, max_samples=MAX_SAMPLES, batch_size=BATCH_SIZE):\n",
    "    global X_train_global, y_train_global, X_val_global, y_val_global, feature_names_master_global, num_class_global, current_best_params\n",
    "\n",
    "    # 1) Collect data (streaming, capped)\n",
    "    X_all, y_all, feature_names_master = collect_samples(feature_files_map, base_path, max_samples=max_samples)\n",
    "    feature_names_master_global = feature_names_master\n",
    "    num_class_global = len(label_encoder.classes_)\n",
    "\n",
    "    # 2) Global train/val/test split\n",
    "    n = X_all.shape[0]\n",
    "    idx = np.arange(n)\n",
    "    rng = np.random.RandomState(RANDOM_SEED)\n",
    "    rng.shuffle(idx)\n",
    "\n",
    "    n_test = int(math.floor(TEST_FRACTION * n))\n",
    "    n_val = int(math.floor(VAL_FRACTION * n))\n",
    "    n_test = max(1, n_test) if n > 2 else 0\n",
    "    n_val = max(1, n_val) if n > 2 else 0\n",
    "    n_train = n - n_val - n_test\n",
    "    if n_train <= 0:\n",
    "        raise RuntimeError(\"Not enough samples for train after split; reduce VAL/TEST fractions or increase MAX_SAMPLES.\")\n",
    "\n",
    "    test_idx = idx[:n_test]\n",
    "    val_idx = idx[n_test:n_test + n_val]\n",
    "    train_idx = idx[n_test + n_val:]\n",
    "\n",
    "    X_train = X_all[train_idx]\n",
    "    y_train = y_all[train_idx]\n",
    "    X_val = X_all[val_idx]\n",
    "    y_val = y_all[val_idx]\n",
    "    X_test = X_all[test_idx]\n",
    "    y_test = y_all[test_idx]\n",
    "\n",
    "    print(f\"[SPLIT] total={n}, train={X_train.shape[0]}, val={X_val.shape[0]}, test={X_test.shape[0]}\")\n",
    "\n",
    "    # expose to BO objective\n",
    "    X_train_global = X_train\n",
    "    y_train_global = y_train\n",
    "    X_val_global = X_val\n",
    "    y_val_global = y_val\n",
    "\n",
    "    booster = None\n",
    "    n_train_rows = X_train.shape[0]\n",
    "    batch_indices = [np.arange(i, min(i + batch_size, n_train_rows)) for i in range(0, n_train_rows, batch_size)]\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\n=== EPOCH {epoch}/{epochs} ===\")\n",
    "        rng.shuffle(batch_indices)\n",
    "        for bi, inds in enumerate(batch_indices, start=1):\n",
    "            Xb = X_train[inds]\n",
    "            yb = y_train[inds]\n",
    "            if Xb.shape[0] == 0:\n",
    "                continue\n",
    "            dtrain = xgb.DMatrix(Xb, label=yb, feature_names=feature_names_master)\n",
    "            booster = xgb.train(\n",
    "                params={**XGB_FIXED_PARAMS, **current_best_params, \"num_class\": num_class_global},\n",
    "                dtrain=dtrain,\n",
    "                num_boost_round=ROUNDS_PER_BATCH,\n",
    "                xgb_model=booster,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "            if bi % 10 == 0:\n",
    "                print(f\"  [epoch {epoch}] processed {bi}/{len(batch_indices)} batches\")\n",
    "\n",
    "        # Bayesian optimization after epoch\n",
    "        print(\"[Bayesian Opt] Running hyperparameter tuning on validation set...\")\n",
    "        t0 = time.time()\n",
    "        res = gp_minimize(\n",
    "            objective,\n",
    "            space,\n",
    "            n_calls=GP_N_CALLS,\n",
    "            n_initial_points=GP_INIT_POINTS,\n",
    "            random_state=RANDOM_SEED,\n",
    "            acq_func=\"EI\",\n",
    "            verbose=False\n",
    "        )\n",
    "        t_elapsed = time.time() - t0\n",
    "        print(f\"[Bayes] Done in {t_elapsed:.1f}s. Best objective value={res.fun:.6f}\")\n",
    "\n",
    "        best_params = {dim.name: val for dim, val in zip(space, res.x)}\n",
    "        current_best_params.update(best_params)\n",
    "        print(f\"[EPOCH {epoch}] Updated best params: {current_best_params}\")\n",
    "\n",
    "    # Final test\n",
    "    if booster is not None:\n",
    "        dtest = xgb.DMatrix(X_test, label=y_test, feature_names=feature_names_master)\n",
    "        preds_prob = booster.predict(dtest)\n",
    "        preds = np.argmax(preds_prob, axis=1)\n",
    "        acc = accuracy_score(y_test, preds)\n",
    "        f1 = f1_score(y_test, preds, average=\"macro\")\n",
    "        print(\"\\n=== FINAL TEST ===\")\n",
    "        print(f\"Accuracy={acc:.4f}  F1={f1:.4f}\")\n",
    "        print(classification_report(y_test, preds, labels=np.arange(num_class_global), target_names=label_encoder.classes_, zero_division=0))\n",
    "    else:\n",
    "        print(\"[FINAL] No booster was trained.\")\n",
    "\n",
    "    return booster, feature_names_master\n",
    "\n",
    "# ------------------------------\n",
    "# Run entrypoint\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    booster_final, feature_names_used = train_boosted_tree_batchwise(feature_files, base_path, epochs=EPOCHS, max_samples=MAX_SAMPLES, batch_size=BATCH_SIZE)\n",
    "    print(\"Done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seawsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
