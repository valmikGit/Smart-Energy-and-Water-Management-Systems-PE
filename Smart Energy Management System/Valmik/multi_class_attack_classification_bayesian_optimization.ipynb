{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3726ec18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COLLECT] Collected 32877238 samples across 5 classes.\n",
      "[BALANCE] Class counts before: {np.int32(0): np.int64(10095091), np.int32(1): np.int64(1314075), np.int32(2): np.int64(188443), np.int32(3): np.int64(329764), np.int32(4): np.int64(20949865)}, using N=188443 per class.\n",
      "[BALANCE] Balanced dataset has 942215 samples total.\n",
      "[SPLIT] train=753773, val=94221, test=94221\n",
      "\n",
      "=== EPOCH 1/3 ===\n",
      "[Bayesian Opt] Running hyperparameter tuning...\n",
      "[EPOCH 1] Updated best params: {'eta': 0.2130355435357968, 'max_depth': np.int64(8), 'subsample': 0.7240191796025336, 'colsample_bytree': 0.7775368807814533}\n",
      "\n",
      "=== EPOCH 2/3 ===\n",
      "[Bayesian Opt] Running hyperparameter tuning...\n",
      "[EPOCH 2] Updated best params: {'eta': 0.2130355435357968, 'max_depth': np.int64(8), 'subsample': 0.7240191796025336, 'colsample_bytree': 0.7775368807814533}\n",
      "\n",
      "=== EPOCH 3/3 ===\n",
      "[Bayesian Opt] Running hyperparameter tuning...\n",
      "[EPOCH 3] Updated best params: {'eta': 0.2130355435357968, 'max_depth': np.int64(8), 'subsample': 0.7240191796025336, 'colsample_bytree': 0.7775368807814533}\n",
      "\n",
      "=== FINAL TEST ===\n",
      "Accuracy=0.7447  F1=0.7419\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "mqtt_bruteforce       1.00      0.99      1.00     19070\n",
      "         normal       0.51      0.66      0.57     18647\n",
      "         scan_A       0.68      0.73      0.71     18741\n",
      "        scan_sU       0.56      0.40      0.47     18781\n",
      "         sparta       1.00      0.94      0.97     18982\n",
      "\n",
      "       accuracy                           0.74     94221\n",
      "      macro avg       0.75      0.74      0.74     94221\n",
      "   weighted avg       0.75      0.74      0.74     94221\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "balanced_xgboost_bayes.py\n",
    "\n",
    "Train an XGBoost model with Bayesian optimization, ensuring class balance:\n",
    " - Streams CSVs to collect data\n",
    " - Balances dataset by downsampling each attack class to the size of the smallest class\n",
    " - Splits into train/val/test\n",
    " - Runs batchwise training with Bayesian hyperparameter optimization after each epoch\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import xgboost as xgb\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "# ------------------------------\n",
    "# Config\n",
    "# ------------------------------\n",
    "base_path = \"./\"\n",
    "folders = {\n",
    "    \"packet\": \"packet_features\",\n",
    "    \"uniflow\": \"uniflow_features\",\n",
    "    \"biflow\": \"biflow_features\",\n",
    "}\n",
    "files = {\n",
    "    \"normal\": \"normal.csv\",\n",
    "    \"sparta\": \"sparta.csv\",\n",
    "    \"scan_A\": \"scan_A.csv\",\n",
    "    \"mqtt_bruteforce\": \"mqtt_bruteforce.csv\",\n",
    "    \"scan_sU\": \"scan_sU.csv\",\n",
    "}\n",
    "\n",
    "def build_filenames(prefix):\n",
    "    return {k: f\"{prefix}_{v}\" for k, v in files.items()}\n",
    "\n",
    "feature_files = {\n",
    "    \"packet\": files,\n",
    "    \"uniflow\": build_filenames(\"uniflow\"),\n",
    "    \"biflow\": build_filenames(\"biflow\"),\n",
    "}\n",
    "\n",
    "# Training params\n",
    "CHUNKSIZE = 2000\n",
    "TRAIN_FRACTION, VAL_FRACTION, TEST_FRACTION = 0.80, 0.10, 0.10\n",
    "EPOCHS = 3\n",
    "ROUNDS_PER_BATCH = 2\n",
    "BATCH_SIZE = 4096\n",
    "GP_N_CALLS = 10\n",
    "GP_INIT_POINTS = 3\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Hyperparameter search space\n",
    "space = [\n",
    "    Real(0.01, 0.3, name=\"eta\"),\n",
    "    Integer(3, 10, name=\"max_depth\"),\n",
    "    Real(0.5, 1.0, name=\"subsample\"),\n",
    "    Real(0.5, 1.0, name=\"colsample_bytree\"),\n",
    "]\n",
    "\n",
    "current_best_params = {\n",
    "    \"eta\": 0.1,\n",
    "    \"max_depth\": 6,\n",
    "    \"subsample\": 1.0,\n",
    "    \"colsample_bytree\": 1.0,\n",
    "}\n",
    "\n",
    "XGB_FIXED_PARAMS = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"verbosity\": 0,\n",
    "}\n",
    "\n",
    "# ------------------------------\n",
    "# Label encoding\n",
    "# ------------------------------\n",
    "attack_type_names = [\"normal\", \"sparta\", \"scan_A\", \"mqtt_bruteforce\", \"scan_sU\"]\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(attack_type_names)\n",
    "\n",
    "# ------------------------------\n",
    "# Data loading utilities\n",
    "# ------------------------------\n",
    "def stream_chunks_for_all_files(feature_files_map, base_path, chunksize=CHUNKSIZE):\n",
    "    \"\"\"Yield (level, key, filepath, chunk_df)\"\"\"\n",
    "    for level, file_dict in feature_files_map.items():\n",
    "        folder_path = os.path.join(base_path, folders[level])\n",
    "        for key, fname in file_dict.items():\n",
    "            fpath = os.path.join(folder_path, fname)\n",
    "            if not os.path.isfile(fpath):\n",
    "                continue\n",
    "            for chunk in pd.read_csv(fpath, chunksize=chunksize, low_memory=False):\n",
    "                yield (level, key, fpath, chunk)\n",
    "\n",
    "def preprocess_chunk(df, file_key, expected_feature_names=None):\n",
    "    \"\"\"\n",
    "    Returns (X_numpy, y_numpy, feature_names_list)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"attack_type\"] = file_key\n",
    "    y_encoded = label_encoder.transform(df[\"attack_type\"].astype(str).values)\n",
    "\n",
    "    numeric_df = df.select_dtypes(include=[np.number]).copy()\n",
    "    for col in [\"label\", \"attack_type\"]:\n",
    "        if col in numeric_df.columns:\n",
    "            numeric_df = numeric_df.drop(columns=[col])\n",
    "\n",
    "    if expected_feature_names is not None:\n",
    "        numeric_df = numeric_df.reindex(columns=expected_feature_names, fill_value=0.0)\n",
    "        feature_names = list(expected_feature_names)\n",
    "    else:\n",
    "        feature_names = list(numeric_df.columns)\n",
    "\n",
    "    numeric_df = numeric_df.fillna(0.0)\n",
    "    X = numeric_df.values.astype(np.float32)\n",
    "    y = y_encoded.astype(np.int32)\n",
    "    return X, y, feature_names\n",
    "\n",
    "def collect_all_samples(feature_files_map, base_path):\n",
    "    \"\"\"Collect ALL samples into memory (for balancing).\"\"\"\n",
    "    X_list, y_list = [], []\n",
    "    feature_names_master = None\n",
    "    for level, key, path, chunk in stream_chunks_for_all_files(feature_files_map, base_path):\n",
    "        if feature_names_master is None:\n",
    "            Xb, yb, feature_names_master = preprocess_chunk(chunk, key, expected_feature_names=None)\n",
    "        else:\n",
    "            Xb, yb, _ = preprocess_chunk(chunk, key, expected_feature_names=feature_names_master)\n",
    "        if Xb.shape[0] == 0:\n",
    "            continue\n",
    "        X_list.append(Xb)\n",
    "        y_list.append(yb)\n",
    "\n",
    "    if not X_list:\n",
    "        raise RuntimeError(\"No data found in dataset.\")\n",
    "\n",
    "    X_all = np.vstack(X_list)\n",
    "    y_all = np.concatenate(y_list)\n",
    "    print(f\"[COLLECT] Collected {X_all.shape[0]} samples across {len(label_encoder.classes_)} classes.\")\n",
    "    return X_all, y_all, feature_names_master\n",
    "\n",
    "def balance_classes(X, y):\n",
    "    \"\"\"Downsample all classes to the size of the smallest class.\"\"\"\n",
    "    class_counts = {cls: np.sum(y == cls) for cls in np.unique(y)}\n",
    "    min_count = min(class_counts.values())\n",
    "    print(f\"[BALANCE] Class counts before: {class_counts}, using N={min_count} per class.\")\n",
    "\n",
    "    X_balanced, y_balanced = [], []\n",
    "    for cls in np.unique(y):\n",
    "        idxs = np.where(y == cls)[0]\n",
    "        chosen = np.random.choice(idxs, size=min_count, replace=False)\n",
    "        X_balanced.append(X[chosen])\n",
    "        y_balanced.append(y[chosen])\n",
    "\n",
    "    X_balanced = np.vstack(X_balanced)\n",
    "    y_balanced = np.concatenate(y_balanced)\n",
    "    print(f\"[BALANCE] Balanced dataset has {X_balanced.shape[0]} samples total.\")\n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "# ------------------------------\n",
    "# Global variables for Bayesian objective\n",
    "# ------------------------------\n",
    "X_train_global = y_train_global = X_val_global = y_val_global = None\n",
    "feature_names_master_global = None\n",
    "num_class_global = None\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(eta, max_depth, subsample, colsample_bytree):\n",
    "    \"\"\"Bayesian optimization objective.\"\"\"\n",
    "    global X_train_global, y_train_global, X_val_global, y_val_global, feature_names_master_global, num_class_global\n",
    "    params = {\n",
    "        **XGB_FIXED_PARAMS,\n",
    "        \"eta\": float(eta),\n",
    "        \"max_depth\": int(max_depth),\n",
    "        \"subsample\": float(subsample),\n",
    "        \"colsample_bytree\": float(colsample_bytree),\n",
    "        \"num_class\": int(num_class_global),\n",
    "    }\n",
    "    dtrain = xgb.DMatrix(X_train_global, label=y_train_global, feature_names=feature_names_master_global)\n",
    "    dval = xgb.DMatrix(X_val_global, label=y_val_global, feature_names=feature_names_master_global)\n",
    "    try:\n",
    "        booster = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=50,\n",
    "            evals=[(dval, \"val\")],\n",
    "            early_stopping_rounds=5,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[BO-OBJ] xgb.train failed: {e}\")\n",
    "        return 1e6\n",
    "    preds = np.argmax(booster.predict(dval), axis=1)\n",
    "    f1 = f1_score(y_val_global, preds, average=\"macro\")\n",
    "    return -f1\n",
    "\n",
    "# ------------------------------\n",
    "# Training pipeline\n",
    "# ------------------------------\n",
    "def train_balanced_xgboost(feature_files_map, base_path, epochs=EPOCHS, batch_size=BATCH_SIZE):\n",
    "    global X_train_global, y_train_global, X_val_global, y_val_global, feature_names_master_global, num_class_global, current_best_params\n",
    "\n",
    "    # 1. Collect and balance dataset\n",
    "    X_all, y_all, feature_names_master = collect_all_samples(feature_files_map, base_path)\n",
    "    X_all, y_all = balance_classes(X_all, y_all)\n",
    "    feature_names_master_global = feature_names_master\n",
    "    num_class_global = len(label_encoder.classes_)\n",
    "\n",
    "    # 2. Train/val/test split\n",
    "    n = X_all.shape[0]\n",
    "    idx = np.arange(n)\n",
    "    rng = np.random.RandomState(RANDOM_SEED)\n",
    "    rng.shuffle(idx)\n",
    "\n",
    "    n_test = int(math.floor(TEST_FRACTION * n))\n",
    "    n_val = int(math.floor(VAL_FRACTION * n))\n",
    "    n_train = n - n_val - n_test\n",
    "    test_idx = idx[:n_test]\n",
    "    val_idx = idx[n_test:n_test+n_val]\n",
    "    train_idx = idx[n_test+n_val:]\n",
    "\n",
    "    X_train, y_train = X_all[train_idx], y_all[train_idx]\n",
    "    X_val, y_val = X_all[val_idx], y_all[val_idx]\n",
    "    X_test, y_test = X_all[test_idx], y_all[test_idx]\n",
    "\n",
    "    print(f\"[SPLIT] train={X_train.shape[0]}, val={X_val.shape[0]}, test={X_test.shape[0]}\")\n",
    "\n",
    "    # 3. Expose for BO\n",
    "    X_train_global, y_train_global = X_train, y_train\n",
    "    X_val_global, y_val_global = X_val, y_val\n",
    "\n",
    "    # 4. Batch training + BO\n",
    "    booster = None\n",
    "    n_train_rows = X_train.shape[0]\n",
    "    batch_indices = [np.arange(i, min(i+batch_size, n_train_rows)) for i in range(0, n_train_rows, batch_size)]\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(f\"\\n=== EPOCH {epoch}/{epochs} ===\")\n",
    "        rng.shuffle(batch_indices)\n",
    "        for bi, inds in enumerate(batch_indices, start=1):\n",
    "            Xb, yb = X_train[inds], y_train[inds]\n",
    "            dtrain = xgb.DMatrix(Xb, label=yb, feature_names=feature_names_master)\n",
    "            booster = xgb.train(\n",
    "                params={**XGB_FIXED_PARAMS, **current_best_params, \"num_class\": num_class_global},\n",
    "                dtrain=dtrain,\n",
    "                num_boost_round=ROUNDS_PER_BATCH,\n",
    "                xgb_model=booster,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "        print(\"[Bayesian Opt] Running hyperparameter tuning...\")\n",
    "        res = gp_minimize(\n",
    "            objective, space,\n",
    "            n_calls=GP_N_CALLS,\n",
    "            n_initial_points=GP_INIT_POINTS,\n",
    "            random_state=RANDOM_SEED,\n",
    "            acq_func=\"EI\"\n",
    "        )\n",
    "        best_params = {dim.name: val for dim, val in zip(space, res.x)}\n",
    "        current_best_params.update(best_params)\n",
    "        print(f\"[EPOCH {epoch}] Updated best params: {current_best_params}\")\n",
    "\n",
    "    # 5. Final evaluation\n",
    "    if booster is not None:\n",
    "        dtest = xgb.DMatrix(X_test, label=y_test, feature_names=feature_names_master)\n",
    "        preds = np.argmax(booster.predict(dtest), axis=1)\n",
    "        acc = accuracy_score(y_test, preds)\n",
    "        f1 = f1_score(y_test, preds, average=\"macro\")\n",
    "        print(\"\\n=== FINAL TEST ===\")\n",
    "        print(f\"Accuracy={acc:.4f}  F1={f1:.4f}\")\n",
    "        print(classification_report(y_test, preds, labels=np.arange(num_class_global), target_names=label_encoder.classes_, zero_division=0))\n",
    "    else:\n",
    "        print(\"[FINAL] No model trained.\")\n",
    "    return booster, feature_names_master\n",
    "\n",
    "# ------------------------------\n",
    "# Entrypoint\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    booster_final, feature_names_used = train_balanced_xgboost(feature_files, base_path, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "    print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seawsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
